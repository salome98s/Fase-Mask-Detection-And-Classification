{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee69f8c",
   "metadata": {},
   "source": [
    "# Siamese Neural Network With Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfbd30c",
   "metadata": {},
   "source": [
    "**Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5e6a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import image as mp_image\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "print(\"Libraries imported - ready to use PyTorch\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa38e096",
   "metadata": {},
   "source": [
    "**Load Image Folder and Sub-folder of Image folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b039bcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with_mask', 'without_mask']\n"
     ]
    }
   ],
   "source": [
    "training_folder_name = 'D:/data/data'\n",
    "img_size = (128,128)\n",
    "classes = sorted(os.listdir(training_folder_name))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490baede",
   "metadata": {},
   "source": [
    "**Image Resize Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db36540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(src_image, size=(128,128), bg_color=\"white\"): \n",
    "    from PIL import Image, ImageOps \n",
    "    src_image.thumbnail(size, Image.ANTIALIAS)\n",
    "    new_image = Image.new(\"RGB\", size, bg_color)\n",
    "    new_image.paste(src_image, (int((size[0] - src_image.size[0]) / 2), int((size[1] - src_image.size[1]) / 2)))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eb1bb3",
   "metadata": {},
   "source": [
    "**Convert Image Data Into Resized Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610541b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming images...\n",
      "processing folder without_mask\n",
      "processing folder with_mask\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Micro Tec\\anaconda3\\lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "training_folder_name = 'D:/data/data'\n",
    "#New location\n",
    "train_folder = 'D:/data/Resized images'\n",
    "size = (128,128)\n",
    "if os.path.exists(train_folder):\n",
    "    shutil.rmtree(train_folder)\n",
    "\n",
    "# Each subfolder in the input folder\n",
    "print('Transforming images...')\n",
    "for root, folders, files in os.walk(training_folder_name):\n",
    "    for sub_folder in folders:\n",
    "        print('processing folder ' + sub_folder)\n",
    "        saveFolder = os.path.join(train_folder,sub_folder)\n",
    "        if not os.path.exists(saveFolder):\n",
    "            os.makedirs(saveFolder)\n",
    "       #Files in sub-folder through loop\n",
    "        file_names = os.listdir(os.path.join(root,sub_folder))\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(root,sub_folder, file_name)\n",
    "            image = Image.open(file_path)\n",
    "            resized_image = resize_image(image, size)\n",
    "            saveAs = os.path.join(saveFolder, file_name)\n",
    "            resized_image.save(saveAs)\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0606f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders ready to read D:/data/Resized images\n"
     ]
    }
   ],
   "source": [
    "#Load Dataset\n",
    "def load_dataset(data_path):\n",
    "    import torch\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    transformation = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomVerticalFlip(0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    full_dataset = torchvision.datasets.ImageFolder(\n",
    "        root=data_path,\n",
    "        transform=transformation\n",
    "    )\n",
    "    train_size = int(0.7 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=50,\n",
    "        num_workers=0,\n",
    "        shuffle=False\n",
    "    )\n",
    "        \n",
    "    return train_loader, test_loader\n",
    "train_folder = 'D:/data/Resized images'\n",
    "train_loader, test_loader = load_dataset(train_folder)\n",
    "batch_size = train_loader.batch_size\n",
    "print(\"Data loaders ready to read\", train_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e735705",
   "metadata": {},
   "source": [
    "**Architechure Siamese Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df6d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (drop): Dropout2d(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=24576, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.drop = nn.Dropout2d(p=0.2)\n",
    "        self.fc = nn.Linear(in_features=32 * 32 * 24, out_features=num_classes)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.pool(self.conv1(x))) \n",
    "        x = F.relu(self.pool(self.conv2(x)))  \n",
    "        x = F.dropout(self.drop(x), training=self.training)\n",
    "        x = x.view(-1, 32 * 32 * 24)\n",
    "        x = self.fc(x)\n",
    "        return torch.log_softmax(x, dim=1)\n",
    "device = \"cpu\"\n",
    "if (torch.cuda.is_available()):\n",
    "    device = \"cuda\"\n",
    "model = Net(num_classes=len(classes)).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aee41c",
   "metadata": {},
   "source": [
    "**Training Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6106d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    print(\"Epoch:\", epoch)\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)          \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_criteria(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('\\tTraining batch {} Loss: {:.6f}'.format(batch_idx + 1, loss.item()))\n",
    "    avg_loss = train_loss / (batch_idx+1)\n",
    "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5ef3a",
   "metadata": {},
   "source": [
    "**Test Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ebe5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for data, target in test_loader:\n",
    "            batch_count += 1\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_criteria(output, target).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "    avg_loss = test_loss / batch_count\n",
    "    print('Validation set: Average loss: {:.6f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        avg_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940167f",
   "metadata": {},
   "source": [
    "**Adam Optimizer For Adjust Weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540c8c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Epoch: 1\n",
      "\tTraining batch 1 Loss: 0.695249\n",
      "\tTraining batch 2 Loss: 9.028457\n",
      "\tTraining batch 3 Loss: 6.555150\n",
      "\tTraining batch 4 Loss: 3.509977\n",
      "\tTraining batch 5 Loss: 0.662957\n",
      "\tTraining batch 6 Loss: 0.592475\n",
      "\tTraining batch 7 Loss: 0.553054\n",
      "\tTraining batch 8 Loss: 0.668545\n",
      "\tTraining batch 9 Loss: 0.823594\n",
      "\tTraining batch 10 Loss: 0.677738\n",
      "\tTraining batch 11 Loss: 0.593815\n",
      "\tTraining batch 12 Loss: 0.566230\n",
      "\tTraining batch 13 Loss: 0.770614\n",
      "\tTraining batch 14 Loss: 0.634316\n",
      "\tTraining batch 15 Loss: 0.633518\n",
      "\tTraining batch 16 Loss: 0.632004\n",
      "\tTraining batch 17 Loss: 0.647053\n",
      "\tTraining batch 18 Loss: 0.638848\n",
      "\tTraining batch 19 Loss: 0.585842\n",
      "\tTraining batch 20 Loss: 0.622177\n",
      "\tTraining batch 21 Loss: 0.704014\n",
      "\tTraining batch 22 Loss: 0.618593\n",
      "\tTraining batch 23 Loss: 0.614187\n",
      "\tTraining batch 24 Loss: 0.678860\n",
      "\tTraining batch 25 Loss: 0.584787\n",
      "\tTraining batch 26 Loss: 0.665603\n",
      "\tTraining batch 27 Loss: 0.571882\n",
      "\tTraining batch 28 Loss: 0.641835\n",
      "\tTraining batch 29 Loss: 0.549760\n",
      "\tTraining batch 30 Loss: 0.561675\n",
      "\tTraining batch 31 Loss: 0.565111\n",
      "\tTraining batch 32 Loss: 0.786326\n",
      "\tTraining batch 33 Loss: 0.508763\n",
      "\tTraining batch 34 Loss: 0.585756\n",
      "\tTraining batch 35 Loss: 0.698234\n",
      "\tTraining batch 36 Loss: 0.691021\n",
      "\tTraining batch 37 Loss: 0.700014\n",
      "\tTraining batch 38 Loss: 0.686207\n",
      "\tTraining batch 39 Loss: 0.637694\n",
      "\tTraining batch 40 Loss: 0.620874\n",
      "\tTraining batch 41 Loss: 0.680229\n",
      "\tTraining batch 42 Loss: 0.674111\n",
      "\tTraining batch 43 Loss: 0.671420\n",
      "\tTraining batch 44 Loss: 0.629356\n",
      "\tTraining batch 45 Loss: 0.594803\n",
      "\tTraining batch 46 Loss: 0.571049\n",
      "\tTraining batch 47 Loss: 1.322023\n",
      "\tTraining batch 48 Loss: 0.808738\n",
      "\tTraining batch 49 Loss: 0.642003\n",
      "\tTraining batch 50 Loss: 0.691164\n",
      "\tTraining batch 51 Loss: 0.690572\n",
      "\tTraining batch 52 Loss: 0.692185\n",
      "\tTraining batch 53 Loss: 0.688254\n",
      "\tTraining batch 54 Loss: 0.697218\n",
      "\tTraining batch 55 Loss: 0.685097\n",
      "\tTraining batch 56 Loss: 0.683764\n",
      "\tTraining batch 57 Loss: 0.690484\n",
      "\tTraining batch 58 Loss: 0.688681\n",
      "\tTraining batch 59 Loss: 0.693194\n",
      "\tTraining batch 60 Loss: 0.687976\n",
      "\tTraining batch 61 Loss: 0.681232\n",
      "\tTraining batch 62 Loss: 0.692010\n",
      "\tTraining batch 63 Loss: 0.684220\n",
      "\tTraining batch 64 Loss: 0.686601\n",
      "\tTraining batch 65 Loss: 0.679751\n",
      "\tTraining batch 66 Loss: 0.686294\n",
      "\tTraining batch 67 Loss: 0.666318\n",
      "\tTraining batch 68 Loss: 0.667810\n",
      "\tTraining batch 69 Loss: 0.681491\n",
      "\tTraining batch 70 Loss: 0.672857\n",
      "\tTraining batch 71 Loss: 0.684515\n",
      "\tTraining batch 72 Loss: 0.645033\n",
      "\tTraining batch 73 Loss: 0.626859\n",
      "\tTraining batch 74 Loss: 0.662984\n",
      "\tTraining batch 75 Loss: 0.662255\n",
      "\tTraining batch 76 Loss: 0.705847\n",
      "\tTraining batch 77 Loss: 0.604827\n",
      "\tTraining batch 78 Loss: 0.562235\n",
      "\tTraining batch 79 Loss: 0.692670\n",
      "\tTraining batch 80 Loss: 0.699501\n",
      "\tTraining batch 81 Loss: 0.631130\n",
      "\tTraining batch 82 Loss: 0.585090\n",
      "\tTraining batch 83 Loss: 0.628759\n",
      "\tTraining batch 84 Loss: 0.561814\n",
      "\tTraining batch 85 Loss: 0.616497\n",
      "\tTraining batch 86 Loss: 0.630379\n",
      "\tTraining batch 87 Loss: 0.614122\n",
      "\tTraining batch 88 Loss: 0.626118\n",
      "\tTraining batch 89 Loss: 0.670955\n",
      "\tTraining batch 90 Loss: 0.699510\n",
      "\tTraining batch 91 Loss: 0.587652\n",
      "\tTraining batch 92 Loss: 0.631820\n",
      "\tTraining batch 93 Loss: 0.624143\n",
      "\tTraining batch 94 Loss: 0.611736\n",
      "\tTraining batch 95 Loss: 0.583092\n",
      "\tTraining batch 96 Loss: 0.572913\n",
      "\tTraining batch 97 Loss: 0.558548\n",
      "\tTraining batch 98 Loss: 0.567887\n",
      "\tTraining batch 99 Loss: 0.668375\n",
      "\tTraining batch 100 Loss: 0.643665\n",
      "\tTraining batch 101 Loss: 0.668822\n",
      "\tTraining batch 102 Loss: 0.627114\n",
      "\tTraining batch 103 Loss: 0.549853\n",
      "\tTraining batch 104 Loss: 0.571239\n",
      "\tTraining batch 105 Loss: 0.579490\n",
      "\tTraining batch 106 Loss: 0.565576\n",
      "Training set: Average loss: 0.812894\n",
      "Validation set: Average loss: 0.555420, Accuracy: 1766/2266 (78%)\n",
      "\n",
      "Epoch: 2\n",
      "\tTraining batch 1 Loss: 0.568002\n",
      "\tTraining batch 2 Loss: 0.518519\n",
      "\tTraining batch 3 Loss: 0.519723\n",
      "\tTraining batch 4 Loss: 0.561407\n",
      "\tTraining batch 5 Loss: 0.578467\n",
      "\tTraining batch 6 Loss: 0.523808\n",
      "\tTraining batch 7 Loss: 0.536237\n",
      "\tTraining batch 8 Loss: 0.708723\n",
      "\tTraining batch 9 Loss: 0.669951\n",
      "\tTraining batch 10 Loss: 0.582321\n",
      "\tTraining batch 11 Loss: 0.629992\n",
      "\tTraining batch 12 Loss: 0.590307\n",
      "\tTraining batch 13 Loss: 0.578271\n",
      "\tTraining batch 14 Loss: 0.553232\n",
      "\tTraining batch 15 Loss: 0.538430\n",
      "\tTraining batch 16 Loss: 0.621302\n",
      "\tTraining batch 17 Loss: 0.593780\n",
      "\tTraining batch 18 Loss: 0.514463\n",
      "\tTraining batch 19 Loss: 0.541374\n",
      "\tTraining batch 20 Loss: 0.494642\n",
      "\tTraining batch 21 Loss: 0.596408\n",
      "\tTraining batch 22 Loss: 0.489435\n",
      "\tTraining batch 23 Loss: 0.614936\n",
      "\tTraining batch 24 Loss: 0.545842\n",
      "\tTraining batch 25 Loss: 0.601632\n",
      "\tTraining batch 26 Loss: 0.531699\n",
      "\tTraining batch 27 Loss: 0.459793\n",
      "\tTraining batch 28 Loss: 0.567789\n",
      "\tTraining batch 29 Loss: 0.529223\n",
      "\tTraining batch 30 Loss: 0.547620\n",
      "\tTraining batch 31 Loss: 0.679425\n",
      "\tTraining batch 32 Loss: 0.599688\n",
      "\tTraining batch 33 Loss: 0.528738\n",
      "\tTraining batch 34 Loss: 0.537380\n",
      "\tTraining batch 35 Loss: 0.532932\n",
      "\tTraining batch 36 Loss: 0.497721\n",
      "\tTraining batch 37 Loss: 0.619033\n",
      "\tTraining batch 38 Loss: 0.709011\n",
      "\tTraining batch 39 Loss: 0.466377\n",
      "\tTraining batch 40 Loss: 0.520706\n",
      "\tTraining batch 41 Loss: 0.535083\n",
      "\tTraining batch 42 Loss: 0.590689\n",
      "\tTraining batch 43 Loss: 0.533426\n",
      "\tTraining batch 44 Loss: 0.459798\n",
      "\tTraining batch 45 Loss: 0.684334\n",
      "\tTraining batch 46 Loss: 0.481129\n",
      "\tTraining batch 47 Loss: 0.447210\n",
      "\tTraining batch 48 Loss: 0.533471\n",
      "\tTraining batch 49 Loss: 0.541609\n",
      "\tTraining batch 50 Loss: 0.523428\n",
      "\tTraining batch 51 Loss: 0.489207\n",
      "\tTraining batch 52 Loss: 0.406929\n",
      "\tTraining batch 53 Loss: 0.449680\n",
      "\tTraining batch 54 Loss: 0.523769\n",
      "\tTraining batch 55 Loss: 0.391594\n",
      "\tTraining batch 56 Loss: 0.802046\n",
      "\tTraining batch 57 Loss: 0.607962\n",
      "\tTraining batch 58 Loss: 0.477678\n",
      "\tTraining batch 59 Loss: 0.606687\n",
      "\tTraining batch 60 Loss: 0.414327\n",
      "\tTraining batch 61 Loss: 0.469819\n",
      "\tTraining batch 62 Loss: 0.566688\n",
      "\tTraining batch 63 Loss: 0.476857\n",
      "\tTraining batch 64 Loss: 0.620872\n",
      "\tTraining batch 65 Loss: 0.581696\n",
      "\tTraining batch 66 Loss: 0.486715\n",
      "\tTraining batch 67 Loss: 0.497452\n",
      "\tTraining batch 68 Loss: 0.506080\n",
      "\tTraining batch 69 Loss: 0.474152\n",
      "\tTraining batch 70 Loss: 0.552882\n",
      "\tTraining batch 71 Loss: 0.565343\n",
      "\tTraining batch 72 Loss: 0.692773\n",
      "\tTraining batch 73 Loss: 0.619749\n",
      "\tTraining batch 74 Loss: 0.535369\n",
      "\tTraining batch 75 Loss: 0.580749\n",
      "\tTraining batch 76 Loss: 0.495221\n",
      "\tTraining batch 77 Loss: 0.530702\n",
      "\tTraining batch 78 Loss: 0.507527\n",
      "\tTraining batch 79 Loss: 0.518843\n",
      "\tTraining batch 80 Loss: 0.524491\n",
      "\tTraining batch 81 Loss: 0.612549\n",
      "\tTraining batch 82 Loss: 0.490414\n",
      "\tTraining batch 83 Loss: 0.423252\n",
      "\tTraining batch 84 Loss: 0.485072\n",
      "\tTraining batch 85 Loss: 0.521576\n",
      "\tTraining batch 86 Loss: 0.573008\n",
      "\tTraining batch 87 Loss: 0.605699\n",
      "\tTraining batch 88 Loss: 0.409738\n",
      "\tTraining batch 89 Loss: 0.442093\n",
      "\tTraining batch 90 Loss: 0.563489\n",
      "\tTraining batch 91 Loss: 0.458576\n",
      "\tTraining batch 92 Loss: 0.487820\n",
      "\tTraining batch 93 Loss: 0.426323\n",
      "\tTraining batch 94 Loss: 0.533924\n",
      "\tTraining batch 95 Loss: 0.492293\n",
      "\tTraining batch 96 Loss: 0.415815\n",
      "\tTraining batch 97 Loss: 0.356692\n",
      "\tTraining batch 98 Loss: 0.410649\n",
      "\tTraining batch 99 Loss: 0.479225\n",
      "\tTraining batch 100 Loss: 0.714383\n",
      "\tTraining batch 101 Loss: 0.641258\n",
      "\tTraining batch 102 Loss: 0.444876\n",
      "\tTraining batch 103 Loss: 0.425953\n",
      "\tTraining batch 104 Loss: 0.442424\n",
      "\tTraining batch 105 Loss: 0.505348\n",
      "\tTraining batch 106 Loss: 0.422766\n",
      "Training set: Average loss: 0.534789\n",
      "Validation set: Average loss: 0.426102, Accuracy: 1865/2266 (82%)\n",
      "\n",
      "Epoch: 3\n",
      "\tTraining batch 1 Loss: 0.373845\n",
      "\tTraining batch 2 Loss: 0.476855\n",
      "\tTraining batch 3 Loss: 0.454122\n",
      "\tTraining batch 4 Loss: 0.411325\n",
      "\tTraining batch 5 Loss: 0.532338\n",
      "\tTraining batch 6 Loss: 0.473572\n",
      "\tTraining batch 7 Loss: 0.470588\n",
      "\tTraining batch 8 Loss: 0.553927\n",
      "\tTraining batch 9 Loss: 0.513005\n",
      "\tTraining batch 10 Loss: 0.513173\n",
      "\tTraining batch 11 Loss: 0.504285\n",
      "\tTraining batch 12 Loss: 0.477113\n",
      "\tTraining batch 13 Loss: 0.492874\n",
      "\tTraining batch 14 Loss: 0.482273\n",
      "\tTraining batch 15 Loss: 0.410498\n",
      "\tTraining batch 16 Loss: 0.478883\n",
      "\tTraining batch 17 Loss: 0.414962\n",
      "\tTraining batch 18 Loss: 0.452347\n",
      "\tTraining batch 19 Loss: 0.469452\n",
      "\tTraining batch 20 Loss: 0.363033\n",
      "\tTraining batch 21 Loss: 0.582338\n",
      "\tTraining batch 22 Loss: 0.348294\n",
      "\tTraining batch 23 Loss: 0.496150\n",
      "\tTraining batch 24 Loss: 0.568591\n",
      "\tTraining batch 25 Loss: 0.601577\n",
      "\tTraining batch 26 Loss: 0.534475\n",
      "\tTraining batch 27 Loss: 0.418848\n",
      "\tTraining batch 28 Loss: 0.454806\n",
      "\tTraining batch 29 Loss: 0.419289\n",
      "\tTraining batch 30 Loss: 0.500659\n",
      "\tTraining batch 31 Loss: 0.687316\n",
      "\tTraining batch 32 Loss: 0.506100\n",
      "\tTraining batch 33 Loss: 0.492036\n",
      "\tTraining batch 34 Loss: 0.574395\n",
      "\tTraining batch 35 Loss: 0.486547\n",
      "\tTraining batch 36 Loss: 0.447438\n",
      "\tTraining batch 37 Loss: 0.373514\n",
      "\tTraining batch 38 Loss: 0.632041\n",
      "\tTraining batch 39 Loss: 0.490258\n",
      "\tTraining batch 40 Loss: 0.396648\n",
      "\tTraining batch 41 Loss: 0.432536\n",
      "\tTraining batch 42 Loss: 0.529153\n",
      "\tTraining batch 43 Loss: 0.574037\n",
      "\tTraining batch 44 Loss: 0.462234\n",
      "\tTraining batch 45 Loss: 0.501317\n",
      "\tTraining batch 46 Loss: 0.355182\n",
      "\tTraining batch 47 Loss: 0.367051\n",
      "\tTraining batch 48 Loss: 0.562546\n",
      "\tTraining batch 49 Loss: 0.557334\n",
      "\tTraining batch 50 Loss: 0.456668\n",
      "\tTraining batch 51 Loss: 0.382974\n",
      "\tTraining batch 52 Loss: 0.276300\n",
      "\tTraining batch 53 Loss: 0.489614\n",
      "\tTraining batch 54 Loss: 0.606316\n",
      "\tTraining batch 55 Loss: 0.373555\n",
      "\tTraining batch 56 Loss: 0.610071\n",
      "\tTraining batch 57 Loss: 0.647413\n",
      "\tTraining batch 58 Loss: 0.639602\n",
      "\tTraining batch 59 Loss: 0.630507\n",
      "\tTraining batch 60 Loss: 0.533635\n",
      "\tTraining batch 61 Loss: 0.643781\n",
      "\tTraining batch 62 Loss: 0.472971\n",
      "\tTraining batch 63 Loss: 0.614755\n",
      "\tTraining batch 64 Loss: 0.601845\n",
      "\tTraining batch 65 Loss: 0.587012\n",
      "\tTraining batch 66 Loss: 0.697427\n",
      "\tTraining batch 67 Loss: 0.535096\n",
      "\tTraining batch 68 Loss: 0.477825\n",
      "\tTraining batch 69 Loss: 0.600133\n",
      "\tTraining batch 70 Loss: 0.515292\n",
      "\tTraining batch 71 Loss: 0.665849\n",
      "\tTraining batch 72 Loss: 0.656164\n",
      "\tTraining batch 73 Loss: 0.587652\n",
      "\tTraining batch 74 Loss: 0.636572\n",
      "\tTraining batch 75 Loss: 0.569650\n",
      "\tTraining batch 76 Loss: 0.570236\n",
      "\tTraining batch 77 Loss: 0.647082\n",
      "\tTraining batch 78 Loss: 0.644419\n",
      "\tTraining batch 79 Loss: 0.585873\n",
      "\tTraining batch 80 Loss: 0.459257\n",
      "\tTraining batch 81 Loss: 0.604607\n",
      "\tTraining batch 82 Loss: 0.515047\n",
      "\tTraining batch 83 Loss: 0.405026\n",
      "\tTraining batch 84 Loss: 0.458668\n",
      "\tTraining batch 85 Loss: 0.668351\n",
      "\tTraining batch 86 Loss: 2.049933\n",
      "\tTraining batch 87 Loss: 0.618153\n",
      "\tTraining batch 88 Loss: 0.589812\n",
      "\tTraining batch 89 Loss: 0.559395\n",
      "\tTraining batch 90 Loss: 0.619842\n",
      "\tTraining batch 91 Loss: 0.614681\n",
      "\tTraining batch 92 Loss: 0.551321\n",
      "\tTraining batch 93 Loss: 0.595913\n",
      "\tTraining batch 94 Loss: 0.738353\n",
      "\tTraining batch 95 Loss: 0.705716\n",
      "\tTraining batch 96 Loss: 0.528375\n",
      "\tTraining batch 97 Loss: 0.669505\n",
      "\tTraining batch 98 Loss: 0.609719\n",
      "\tTraining batch 99 Loss: 0.820289\n",
      "\tTraining batch 100 Loss: 0.630543\n",
      "\tTraining batch 101 Loss: 0.659720\n",
      "\tTraining batch 102 Loss: 0.636653\n",
      "\tTraining batch 103 Loss: 0.827290\n",
      "\tTraining batch 104 Loss: 0.496361\n",
      "\tTraining batch 105 Loss: 0.554833\n",
      "\tTraining batch 106 Loss: 0.644760\n",
      "Training set: Average loss: 0.551506\n",
      "Validation set: Average loss: 0.569913, Accuracy: 1584/2266 (70%)\n",
      "\n",
      "Epoch: 4\n",
      "\tTraining batch 1 Loss: 0.618368\n",
      "\tTraining batch 2 Loss: 0.494561\n",
      "\tTraining batch 3 Loss: 0.581525\n",
      "\tTraining batch 4 Loss: 0.514439\n",
      "\tTraining batch 5 Loss: 0.549516\n",
      "\tTraining batch 6 Loss: 0.616218\n",
      "\tTraining batch 7 Loss: 0.657627\n",
      "\tTraining batch 8 Loss: 0.665077\n",
      "\tTraining batch 9 Loss: 0.650443\n",
      "\tTraining batch 10 Loss: 0.640103\n",
      "\tTraining batch 11 Loss: 0.710133\n",
      "\tTraining batch 12 Loss: 0.586961\n",
      "\tTraining batch 13 Loss: 0.682233\n",
      "\tTraining batch 14 Loss: 0.676506\n",
      "\tTraining batch 15 Loss: 0.593235\n",
      "\tTraining batch 16 Loss: 0.628239\n",
      "\tTraining batch 17 Loss: 0.641743\n",
      "\tTraining batch 18 Loss: 0.687912\n",
      "\tTraining batch 19 Loss: 0.763838\n",
      "\tTraining batch 20 Loss: 0.678062\n",
      "\tTraining batch 21 Loss: 0.687822\n",
      "\tTraining batch 22 Loss: 0.564186\n",
      "\tTraining batch 23 Loss: 0.557245\n",
      "\tTraining batch 24 Loss: 0.629877\n",
      "\tTraining batch 25 Loss: 0.615062\n",
      "\tTraining batch 26 Loss: 0.612532\n",
      "\tTraining batch 27 Loss: 0.617694\n",
      "\tTraining batch 28 Loss: 0.580966\n",
      "\tTraining batch 29 Loss: 0.560128\n",
      "\tTraining batch 30 Loss: 0.646202\n",
      "\tTraining batch 31 Loss: 0.741668\n",
      "\tTraining batch 32 Loss: 0.595794\n",
      "\tTraining batch 33 Loss: 0.652209\n",
      "\tTraining batch 34 Loss: 0.741236\n",
      "\tTraining batch 35 Loss: 0.611727\n",
      "\tTraining batch 36 Loss: 0.605015\n",
      "\tTraining batch 37 Loss: 0.475767\n",
      "\tTraining batch 38 Loss: 0.634586\n",
      "\tTraining batch 39 Loss: 0.455398\n",
      "\tTraining batch 40 Loss: 0.465118\n",
      "\tTraining batch 41 Loss: 0.545538\n",
      "\tTraining batch 42 Loss: 0.550523\n",
      "\tTraining batch 43 Loss: 0.534169\n",
      "\tTraining batch 44 Loss: 0.559837\n",
      "\tTraining batch 45 Loss: 0.639633\n",
      "\tTraining batch 46 Loss: 0.542984\n",
      "\tTraining batch 47 Loss: 0.619588\n",
      "\tTraining batch 48 Loss: 0.607555\n",
      "\tTraining batch 49 Loss: 0.616079\n",
      "\tTraining batch 50 Loss: 0.575452\n",
      "\tTraining batch 51 Loss: 0.564412\n",
      "\tTraining batch 52 Loss: 0.484641\n",
      "\tTraining batch 53 Loss: 0.545592\n",
      "\tTraining batch 54 Loss: 0.706801\n",
      "\tTraining batch 55 Loss: 0.476732\n",
      "\tTraining batch 56 Loss: 0.622795\n",
      "\tTraining batch 57 Loss: 0.602298\n",
      "\tTraining batch 58 Loss: 0.526291\n",
      "\tTraining batch 59 Loss: 0.529062\n",
      "\tTraining batch 60 Loss: 0.476309\n",
      "\tTraining batch 61 Loss: 0.635535\n",
      "\tTraining batch 62 Loss: 0.523114\n",
      "\tTraining batch 63 Loss: 0.628979\n",
      "\tTraining batch 64 Loss: 0.617555\n",
      "\tTraining batch 65 Loss: 0.569850\n",
      "\tTraining batch 66 Loss: 0.521083\n",
      "\tTraining batch 67 Loss: 0.511277\n",
      "\tTraining batch 68 Loss: 0.527989\n",
      "\tTraining batch 69 Loss: 0.622314\n",
      "\tTraining batch 70 Loss: 0.488579\n",
      "\tTraining batch 71 Loss: 0.553626\n",
      "\tTraining batch 72 Loss: 0.657195\n",
      "\tTraining batch 73 Loss: 0.523827\n",
      "\tTraining batch 74 Loss: 0.504631\n",
      "\tTraining batch 75 Loss: 0.634924\n",
      "\tTraining batch 76 Loss: 0.569499\n",
      "\tTraining batch 77 Loss: 0.576985\n",
      "\tTraining batch 78 Loss: 0.606429\n",
      "\tTraining batch 79 Loss: 0.497070\n",
      "\tTraining batch 80 Loss: 0.530808\n",
      "\tTraining batch 81 Loss: 0.590037\n",
      "\tTraining batch 82 Loss: 0.524765\n",
      "\tTraining batch 83 Loss: 0.484971\n",
      "\tTraining batch 84 Loss: 0.538198\n",
      "\tTraining batch 85 Loss: 0.539421\n",
      "\tTraining batch 86 Loss: 0.582582\n",
      "\tTraining batch 87 Loss: 0.500393\n",
      "\tTraining batch 88 Loss: 0.580997\n",
      "\tTraining batch 89 Loss: 0.630192\n",
      "\tTraining batch 90 Loss: 0.845239\n",
      "\tTraining batch 91 Loss: 0.528939\n",
      "\tTraining batch 92 Loss: 0.472534\n",
      "\tTraining batch 93 Loss: 0.511347\n",
      "\tTraining batch 94 Loss: 0.576791\n",
      "\tTraining batch 95 Loss: 0.548133\n",
      "\tTraining batch 96 Loss: 0.397632\n",
      "\tTraining batch 97 Loss: 0.565293\n",
      "\tTraining batch 98 Loss: 0.526602\n",
      "\tTraining batch 99 Loss: 0.597749\n",
      "\tTraining batch 100 Loss: 0.512520\n",
      "\tTraining batch 101 Loss: 0.561964\n",
      "\tTraining batch 102 Loss: 0.415435\n",
      "\tTraining batch 103 Loss: 0.469834\n",
      "\tTraining batch 104 Loss: 0.602743\n",
      "\tTraining batch 105 Loss: 0.456404\n",
      "\tTraining batch 106 Loss: 0.362348\n",
      "Training set: Average loss: 0.577713\n",
      "Validation set: Average loss: 0.460313, Accuracy: 1796/2266 (79%)\n",
      "\n",
      "Epoch: 5\n",
      "\tTraining batch 1 Loss: 0.460749\n",
      "\tTraining batch 2 Loss: 0.333547\n",
      "\tTraining batch 3 Loss: 0.450519\n",
      "\tTraining batch 4 Loss: 0.325171\n",
      "\tTraining batch 5 Loss: 0.543940\n",
      "\tTraining batch 6 Loss: 0.461872\n",
      "\tTraining batch 7 Loss: 0.443684\n",
      "\tTraining batch 8 Loss: 0.648576\n",
      "\tTraining batch 9 Loss: 0.543430\n",
      "\tTraining batch 10 Loss: 0.676207\n",
      "\tTraining batch 11 Loss: 0.402160\n",
      "\tTraining batch 12 Loss: 0.521261\n",
      "\tTraining batch 13 Loss: 0.466224\n",
      "\tTraining batch 14 Loss: 0.544692\n",
      "\tTraining batch 15 Loss: 0.501147\n",
      "\tTraining batch 16 Loss: 0.530772\n",
      "\tTraining batch 17 Loss: 0.569331\n",
      "\tTraining batch 18 Loss: 0.537237\n",
      "\tTraining batch 19 Loss: 0.481499\n",
      "\tTraining batch 20 Loss: 0.436314\n",
      "\tTraining batch 21 Loss: 0.561330\n",
      "\tTraining batch 22 Loss: 0.486373\n",
      "\tTraining batch 23 Loss: 0.483511\n",
      "\tTraining batch 24 Loss: 0.468317\n",
      "\tTraining batch 25 Loss: 0.518430\n",
      "\tTraining batch 26 Loss: 0.631185\n",
      "\tTraining batch 27 Loss: 0.474819\n",
      "\tTraining batch 28 Loss: 0.492986\n",
      "\tTraining batch 29 Loss: 0.440290\n",
      "\tTraining batch 30 Loss: 0.447810\n",
      "\tTraining batch 31 Loss: 0.634297\n",
      "\tTraining batch 32 Loss: 0.564957\n",
      "\tTraining batch 33 Loss: 0.576206\n",
      "\tTraining batch 34 Loss: 0.562344\n",
      "\tTraining batch 35 Loss: 0.492301\n",
      "\tTraining batch 36 Loss: 0.458079\n",
      "\tTraining batch 37 Loss: 0.470742\n",
      "\tTraining batch 38 Loss: 0.462430\n",
      "\tTraining batch 39 Loss: 0.391390\n",
      "\tTraining batch 40 Loss: 0.467861\n",
      "\tTraining batch 41 Loss: 0.566894\n",
      "\tTraining batch 42 Loss: 0.557809\n",
      "\tTraining batch 43 Loss: 0.465333\n",
      "\tTraining batch 44 Loss: 0.393594\n",
      "\tTraining batch 45 Loss: 0.508829\n",
      "\tTraining batch 46 Loss: 0.385466\n",
      "\tTraining batch 47 Loss: 0.442255\n",
      "\tTraining batch 48 Loss: 0.437220\n",
      "\tTraining batch 49 Loss: 0.693244\n",
      "\tTraining batch 50 Loss: 0.495504\n",
      "\tTraining batch 51 Loss: 0.475177\n",
      "\tTraining batch 52 Loss: 0.419250\n",
      "\tTraining batch 53 Loss: 0.582213\n",
      "\tTraining batch 54 Loss: 0.476118\n",
      "\tTraining batch 55 Loss: 0.465033\n",
      "\tTraining batch 56 Loss: 0.466893\n",
      "\tTraining batch 57 Loss: 0.498961\n",
      "\tTraining batch 58 Loss: 0.619125\n",
      "\tTraining batch 59 Loss: 0.470735\n",
      "\tTraining batch 60 Loss: 0.449092\n",
      "\tTraining batch 61 Loss: 0.459911\n",
      "\tTraining batch 62 Loss: 0.427342\n",
      "\tTraining batch 63 Loss: 0.456853\n",
      "\tTraining batch 64 Loss: 0.586775\n",
      "\tTraining batch 65 Loss: 0.493143\n",
      "\tTraining batch 66 Loss: 0.405499\n",
      "\tTraining batch 67 Loss: 0.451058\n",
      "\tTraining batch 68 Loss: 0.445865\n",
      "\tTraining batch 69 Loss: 0.529750\n",
      "\tTraining batch 70 Loss: 0.535220\n",
      "\tTraining batch 71 Loss: 0.470790\n",
      "\tTraining batch 72 Loss: 0.558233\n",
      "\tTraining batch 73 Loss: 0.598370\n",
      "\tTraining batch 74 Loss: 0.441407\n",
      "\tTraining batch 75 Loss: 0.577685\n",
      "\tTraining batch 76 Loss: 0.477686\n",
      "\tTraining batch 77 Loss: 0.514143\n",
      "\tTraining batch 78 Loss: 0.519826\n",
      "\tTraining batch 79 Loss: 0.584840\n",
      "\tTraining batch 80 Loss: 0.418868\n",
      "\tTraining batch 81 Loss: 0.519468\n",
      "\tTraining batch 82 Loss: 0.409776\n",
      "\tTraining batch 83 Loss: 0.395505\n",
      "\tTraining batch 84 Loss: 0.420169\n",
      "\tTraining batch 85 Loss: 0.427177\n",
      "\tTraining batch 86 Loss: 0.605164\n",
      "\tTraining batch 87 Loss: 0.545983\n",
      "\tTraining batch 88 Loss: 0.335518\n",
      "\tTraining batch 89 Loss: 0.439674\n",
      "\tTraining batch 90 Loss: 0.547786\n",
      "\tTraining batch 91 Loss: 0.481230\n",
      "\tTraining batch 92 Loss: 0.448755\n",
      "\tTraining batch 93 Loss: 0.417462\n",
      "\tTraining batch 94 Loss: 0.475195\n",
      "\tTraining batch 95 Loss: 0.384819\n",
      "\tTraining batch 96 Loss: 0.422329\n",
      "\tTraining batch 97 Loss: 0.447287\n",
      "\tTraining batch 98 Loss: 0.383373\n",
      "\tTraining batch 99 Loss: 0.422343\n",
      "\tTraining batch 100 Loss: 0.546743\n",
      "\tTraining batch 101 Loss: 0.555497\n",
      "\tTraining batch 102 Loss: 0.411697\n",
      "\tTraining batch 103 Loss: 0.467763\n",
      "\tTraining batch 104 Loss: 0.515099\n",
      "\tTraining batch 105 Loss: 0.419588\n",
      "\tTraining batch 106 Loss: 0.445944\n",
      "Training set: Average loss: 0.487522\n",
      "Validation set: Average loss: 0.393368, Accuracy: 1880/2266 (83%)\n",
      "\n",
      "Epoch: 6\n",
      "\tTraining batch 1 Loss: 0.337311\n",
      "\tTraining batch 2 Loss: 0.354418\n",
      "\tTraining batch 3 Loss: 0.448010\n",
      "\tTraining batch 4 Loss: 0.460741\n",
      "\tTraining batch 5 Loss: 0.432129\n",
      "\tTraining batch 6 Loss: 0.398096\n",
      "\tTraining batch 7 Loss: 0.498819\n",
      "\tTraining batch 8 Loss: 0.512073\n",
      "\tTraining batch 9 Loss: 0.544125\n",
      "\tTraining batch 10 Loss: 0.507954\n",
      "\tTraining batch 11 Loss: 0.384573\n",
      "\tTraining batch 12 Loss: 0.414534\n",
      "\tTraining batch 13 Loss: 0.511049\n",
      "\tTraining batch 14 Loss: 0.553094\n",
      "\tTraining batch 15 Loss: 0.441178\n",
      "\tTraining batch 16 Loss: 0.409358\n",
      "\tTraining batch 17 Loss: 0.453664\n",
      "\tTraining batch 18 Loss: 0.444527\n",
      "\tTraining batch 19 Loss: 0.433377\n",
      "\tTraining batch 20 Loss: 0.356963\n",
      "\tTraining batch 21 Loss: 0.446748\n",
      "\tTraining batch 22 Loss: 0.267595\n",
      "\tTraining batch 23 Loss: 0.397492\n",
      "\tTraining batch 24 Loss: 0.408998\n",
      "\tTraining batch 25 Loss: 0.447915\n",
      "\tTraining batch 26 Loss: 0.494711\n",
      "\tTraining batch 27 Loss: 0.451076\n",
      "\tTraining batch 28 Loss: 0.517834\n",
      "\tTraining batch 29 Loss: 0.411552\n",
      "\tTraining batch 30 Loss: 0.461627\n",
      "\tTraining batch 31 Loss: 0.602415\n",
      "\tTraining batch 32 Loss: 0.497628\n",
      "\tTraining batch 33 Loss: 0.439665\n",
      "\tTraining batch 34 Loss: 0.537332\n",
      "\tTraining batch 35 Loss: 0.489578\n",
      "\tTraining batch 36 Loss: 0.447404\n",
      "\tTraining batch 37 Loss: 0.267629\n",
      "\tTraining batch 38 Loss: 0.450685\n",
      "\tTraining batch 39 Loss: 0.413486\n",
      "\tTraining batch 40 Loss: 0.537275\n",
      "\tTraining batch 41 Loss: 0.542488\n",
      "\tTraining batch 42 Loss: 0.610727\n",
      "\tTraining batch 43 Loss: 0.420947\n",
      "\tTraining batch 44 Loss: 0.504140\n",
      "\tTraining batch 45 Loss: 0.501736\n",
      "\tTraining batch 46 Loss: 0.449347\n",
      "\tTraining batch 47 Loss: 0.488718\n",
      "\tTraining batch 48 Loss: 0.465591\n",
      "\tTraining batch 49 Loss: 0.533090\n",
      "\tTraining batch 50 Loss: 0.480715\n",
      "\tTraining batch 51 Loss: 0.521815\n",
      "\tTraining batch 52 Loss: 0.271398\n",
      "\tTraining batch 53 Loss: 0.415050\n",
      "\tTraining batch 54 Loss: 0.508797\n",
      "\tTraining batch 55 Loss: 0.374163\n",
      "\tTraining batch 56 Loss: 0.480772\n",
      "\tTraining batch 57 Loss: 0.499379\n",
      "\tTraining batch 58 Loss: 0.524889\n",
      "\tTraining batch 59 Loss: 0.452850\n",
      "\tTraining batch 60 Loss: 0.382226\n",
      "\tTraining batch 61 Loss: 0.473698\n",
      "\tTraining batch 62 Loss: 0.326484\n",
      "\tTraining batch 63 Loss: 0.559776\n",
      "\tTraining batch 64 Loss: 0.589396\n",
      "\tTraining batch 65 Loss: 0.459933\n",
      "\tTraining batch 66 Loss: 0.431333\n",
      "\tTraining batch 67 Loss: 0.455156\n",
      "\tTraining batch 68 Loss: 0.457310\n",
      "\tTraining batch 69 Loss: 0.528028\n",
      "\tTraining batch 70 Loss: 0.436860\n",
      "\tTraining batch 71 Loss: 0.503588\n",
      "\tTraining batch 72 Loss: 0.493118\n",
      "\tTraining batch 73 Loss: 0.359965\n",
      "\tTraining batch 74 Loss: 0.383758\n",
      "\tTraining batch 75 Loss: 0.461559\n",
      "\tTraining batch 76 Loss: 0.346462\n",
      "\tTraining batch 77 Loss: 0.507502\n",
      "\tTraining batch 78 Loss: 0.466900\n",
      "\tTraining batch 79 Loss: 0.531550\n",
      "\tTraining batch 80 Loss: 0.441791\n",
      "\tTraining batch 81 Loss: 0.525192\n",
      "\tTraining batch 82 Loss: 0.438363\n",
      "\tTraining batch 83 Loss: 0.376679\n",
      "\tTraining batch 84 Loss: 0.377208\n",
      "\tTraining batch 85 Loss: 0.444556\n",
      "\tTraining batch 86 Loss: 0.589771\n",
      "\tTraining batch 87 Loss: 0.416956\n",
      "\tTraining batch 88 Loss: 0.301942\n",
      "\tTraining batch 89 Loss: 0.351717\n",
      "\tTraining batch 90 Loss: 0.535330\n",
      "\tTraining batch 91 Loss: 0.423429\n",
      "\tTraining batch 92 Loss: 0.390434\n",
      "\tTraining batch 93 Loss: 0.406814\n",
      "\tTraining batch 94 Loss: 0.383306\n",
      "\tTraining batch 95 Loss: 0.316439\n",
      "\tTraining batch 96 Loss: 0.205449\n",
      "\tTraining batch 97 Loss: 0.366013\n",
      "\tTraining batch 98 Loss: 0.349421\n",
      "\tTraining batch 99 Loss: 0.484160\n",
      "\tTraining batch 100 Loss: 0.457074\n",
      "\tTraining batch 101 Loss: 0.540805\n",
      "\tTraining batch 102 Loss: 0.420967\n",
      "\tTraining batch 103 Loss: 0.549012\n",
      "\tTraining batch 104 Loss: 0.428526\n",
      "\tTraining batch 105 Loss: 0.447078\n",
      "\tTraining batch 106 Loss: 0.369706\n",
      "Training set: Average loss: 0.447396\n",
      "Validation set: Average loss: 0.352764, Accuracy: 1947/2266 (86%)\n",
      "\n",
      "Epoch: 7\n",
      "\tTraining batch 1 Loss: 0.368298\n",
      "\tTraining batch 2 Loss: 0.300221\n",
      "\tTraining batch 3 Loss: 0.441994\n",
      "\tTraining batch 4 Loss: 0.480476\n",
      "\tTraining batch 5 Loss: 0.262746\n",
      "\tTraining batch 6 Loss: 0.376271\n",
      "\tTraining batch 7 Loss: 0.365318\n",
      "\tTraining batch 8 Loss: 0.480162\n",
      "\tTraining batch 9 Loss: 0.454735\n",
      "\tTraining batch 10 Loss: 0.694731\n",
      "\tTraining batch 11 Loss: 0.415681\n",
      "\tTraining batch 12 Loss: 0.324910\n",
      "\tTraining batch 13 Loss: 0.292822\n",
      "\tTraining batch 14 Loss: 0.398852\n",
      "\tTraining batch 15 Loss: 0.423217\n",
      "\tTraining batch 16 Loss: 0.402938\n",
      "\tTraining batch 17 Loss: 0.312203\n",
      "\tTraining batch 18 Loss: 0.479820\n",
      "\tTraining batch 19 Loss: 0.452072\n",
      "\tTraining batch 20 Loss: 0.450604\n",
      "\tTraining batch 21 Loss: 0.655264\n",
      "\tTraining batch 22 Loss: 0.262135\n",
      "\tTraining batch 23 Loss: 0.356078\n",
      "\tTraining batch 24 Loss: 0.429890\n",
      "\tTraining batch 25 Loss: 0.486618\n",
      "\tTraining batch 26 Loss: 0.543508\n",
      "\tTraining batch 27 Loss: 0.489938\n",
      "\tTraining batch 28 Loss: 0.465130\n",
      "\tTraining batch 29 Loss: 0.405502\n",
      "\tTraining batch 30 Loss: 0.426768\n",
      "\tTraining batch 31 Loss: 0.487527\n",
      "\tTraining batch 32 Loss: 0.364176\n",
      "\tTraining batch 33 Loss: 0.380427\n",
      "\tTraining batch 34 Loss: 0.510958\n",
      "\tTraining batch 35 Loss: 0.423742\n",
      "\tTraining batch 36 Loss: 0.424055\n",
      "\tTraining batch 37 Loss: 0.326511\n",
      "\tTraining batch 38 Loss: 0.541096\n",
      "\tTraining batch 39 Loss: 0.364768\n",
      "\tTraining batch 40 Loss: 0.371991\n",
      "\tTraining batch 41 Loss: 0.445074\n",
      "\tTraining batch 42 Loss: 0.438406\n",
      "\tTraining batch 43 Loss: 0.471763\n",
      "\tTraining batch 44 Loss: 0.413074\n",
      "\tTraining batch 45 Loss: 0.610077\n",
      "\tTraining batch 46 Loss: 0.246017\n",
      "\tTraining batch 47 Loss: 0.499369\n",
      "\tTraining batch 48 Loss: 0.424430\n",
      "\tTraining batch 49 Loss: 0.387878\n",
      "\tTraining batch 50 Loss: 0.345459\n",
      "\tTraining batch 51 Loss: 0.337523\n",
      "\tTraining batch 52 Loss: 0.393180\n",
      "\tTraining batch 53 Loss: 0.291177\n",
      "\tTraining batch 54 Loss: 0.507006\n",
      "\tTraining batch 55 Loss: 0.280621\n",
      "\tTraining batch 56 Loss: 0.413201\n",
      "\tTraining batch 57 Loss: 0.672421\n",
      "\tTraining batch 58 Loss: 0.528131\n",
      "\tTraining batch 59 Loss: 0.492296\n",
      "\tTraining batch 60 Loss: 0.457349\n",
      "\tTraining batch 61 Loss: 0.514872\n",
      "\tTraining batch 62 Loss: 0.356282\n",
      "\tTraining batch 63 Loss: 0.427476\n",
      "\tTraining batch 64 Loss: 0.473737\n",
      "\tTraining batch 65 Loss: 0.429454\n",
      "\tTraining batch 66 Loss: 0.265292\n",
      "\tTraining batch 67 Loss: 0.417988\n",
      "\tTraining batch 68 Loss: 0.454655\n",
      "\tTraining batch 69 Loss: 0.350573\n",
      "\tTraining batch 70 Loss: 0.511519\n",
      "\tTraining batch 71 Loss: 0.364348\n",
      "\tTraining batch 72 Loss: 0.607096\n",
      "\tTraining batch 73 Loss: 0.427959\n",
      "\tTraining batch 74 Loss: 0.462716\n",
      "\tTraining batch 75 Loss: 0.467373\n",
      "\tTraining batch 76 Loss: 0.444914\n",
      "\tTraining batch 77 Loss: 0.414779\n",
      "\tTraining batch 78 Loss: 0.362055\n",
      "\tTraining batch 79 Loss: 0.425313\n",
      "\tTraining batch 80 Loss: 0.432405\n",
      "\tTraining batch 81 Loss: 0.496197\n",
      "\tTraining batch 82 Loss: 0.316272\n",
      "\tTraining batch 83 Loss: 0.232688\n",
      "\tTraining batch 84 Loss: 0.403204\n",
      "\tTraining batch 85 Loss: 0.470229\n",
      "\tTraining batch 86 Loss: 0.376326\n",
      "\tTraining batch 87 Loss: 0.529814\n",
      "\tTraining batch 88 Loss: 0.270416\n",
      "\tTraining batch 89 Loss: 0.281478\n",
      "\tTraining batch 90 Loss: 0.485172\n",
      "\tTraining batch 91 Loss: 0.304165\n",
      "\tTraining batch 92 Loss: 0.272341\n",
      "\tTraining batch 93 Loss: 0.328907\n",
      "\tTraining batch 94 Loss: 0.299194\n",
      "\tTraining batch 95 Loss: 0.278049\n",
      "\tTraining batch 96 Loss: 0.284013\n",
      "\tTraining batch 97 Loss: 0.410308\n",
      "\tTraining batch 98 Loss: 0.310556\n",
      "\tTraining batch 99 Loss: 0.522630\n",
      "\tTraining batch 100 Loss: 0.427073\n",
      "\tTraining batch 101 Loss: 0.471814\n",
      "\tTraining batch 102 Loss: 0.285604\n",
      "\tTraining batch 103 Loss: 0.499104\n",
      "\tTraining batch 104 Loss: 0.395288\n",
      "\tTraining batch 105 Loss: 0.446262\n",
      "\tTraining batch 106 Loss: 0.280791\n",
      "Training set: Average loss: 0.413560\n",
      "Validation set: Average loss: 0.310531, Accuracy: 1966/2266 (87%)\n",
      "\n",
      "Epoch: 8\n",
      "\tTraining batch 1 Loss: 0.267357\n",
      "\tTraining batch 2 Loss: 0.392135\n",
      "\tTraining batch 3 Loss: 0.381750\n",
      "\tTraining batch 4 Loss: 0.337304\n",
      "\tTraining batch 5 Loss: 0.345793\n",
      "\tTraining batch 6 Loss: 0.388904\n",
      "\tTraining batch 7 Loss: 0.292510\n",
      "\tTraining batch 8 Loss: 0.472254\n",
      "\tTraining batch 9 Loss: 0.509767\n",
      "\tTraining batch 10 Loss: 0.483586\n",
      "\tTraining batch 11 Loss: 0.367529\n",
      "\tTraining batch 12 Loss: 0.388001\n",
      "\tTraining batch 13 Loss: 0.421862\n",
      "\tTraining batch 14 Loss: 0.445259\n",
      "\tTraining batch 15 Loss: 0.432934\n",
      "\tTraining batch 16 Loss: 0.420767\n",
      "\tTraining batch 17 Loss: 0.447696\n",
      "\tTraining batch 18 Loss: 0.471413\n",
      "\tTraining batch 19 Loss: 0.459205\n",
      "\tTraining batch 20 Loss: 0.391505\n",
      "\tTraining batch 21 Loss: 0.301851\n",
      "\tTraining batch 22 Loss: 0.372575\n",
      "\tTraining batch 23 Loss: 0.400074\n",
      "\tTraining batch 24 Loss: 0.451837\n",
      "\tTraining batch 25 Loss: 0.545599\n",
      "\tTraining batch 26 Loss: 0.459299\n",
      "\tTraining batch 27 Loss: 0.423162\n",
      "\tTraining batch 28 Loss: 0.342891\n",
      "\tTraining batch 29 Loss: 0.319945\n",
      "\tTraining batch 30 Loss: 0.421405\n",
      "\tTraining batch 31 Loss: 0.413286\n",
      "\tTraining batch 32 Loss: 0.411774\n",
      "\tTraining batch 33 Loss: 0.279550\n",
      "\tTraining batch 34 Loss: 0.363792\n",
      "\tTraining batch 35 Loss: 0.333888\n",
      "\tTraining batch 36 Loss: 0.351941\n",
      "\tTraining batch 37 Loss: 0.400982\n",
      "\tTraining batch 38 Loss: 0.425480\n",
      "\tTraining batch 39 Loss: 0.252009\n",
      "\tTraining batch 40 Loss: 0.340771\n",
      "\tTraining batch 41 Loss: 0.508076\n",
      "\tTraining batch 42 Loss: 0.499864\n",
      "\tTraining batch 43 Loss: 0.542217\n",
      "\tTraining batch 44 Loss: 0.335313\n",
      "\tTraining batch 45 Loss: 0.617450\n",
      "\tTraining batch 46 Loss: 0.314169\n",
      "\tTraining batch 47 Loss: 0.386675\n",
      "\tTraining batch 48 Loss: 0.438432\n",
      "\tTraining batch 49 Loss: 0.615673\n",
      "\tTraining batch 50 Loss: 0.422991\n",
      "\tTraining batch 51 Loss: 0.378049\n",
      "\tTraining batch 52 Loss: 0.253519\n",
      "\tTraining batch 53 Loss: 0.382030\n",
      "\tTraining batch 54 Loss: 0.491413\n",
      "\tTraining batch 55 Loss: 0.247195\n",
      "\tTraining batch 56 Loss: 0.466779\n",
      "\tTraining batch 57 Loss: 0.600634\n",
      "\tTraining batch 58 Loss: 0.481550\n",
      "\tTraining batch 59 Loss: 0.583395\n",
      "\tTraining batch 60 Loss: 0.383813\n",
      "\tTraining batch 61 Loss: 0.525566\n",
      "\tTraining batch 62 Loss: 0.445997\n",
      "\tTraining batch 63 Loss: 0.344913\n",
      "\tTraining batch 64 Loss: 0.526273\n",
      "\tTraining batch 65 Loss: 0.354627\n",
      "\tTraining batch 66 Loss: 0.445903\n",
      "\tTraining batch 67 Loss: 0.229087\n",
      "\tTraining batch 68 Loss: 0.480918\n",
      "\tTraining batch 69 Loss: 0.454772\n",
      "\tTraining batch 70 Loss: 0.319628\n",
      "\tTraining batch 71 Loss: 0.422170\n",
      "\tTraining batch 72 Loss: 0.424983\n",
      "\tTraining batch 73 Loss: 0.479966\n",
      "\tTraining batch 74 Loss: 0.320237\n",
      "\tTraining batch 75 Loss: 0.442281\n",
      "\tTraining batch 76 Loss: 0.360420\n",
      "\tTraining batch 77 Loss: 0.347334\n",
      "\tTraining batch 78 Loss: 0.257536\n",
      "\tTraining batch 79 Loss: 0.409953\n",
      "\tTraining batch 80 Loss: 0.443228\n",
      "\tTraining batch 81 Loss: 0.465175\n",
      "\tTraining batch 82 Loss: 0.281726\n",
      "\tTraining batch 83 Loss: 0.260920\n",
      "\tTraining batch 84 Loss: 0.360403\n",
      "\tTraining batch 85 Loss: 0.338286\n",
      "\tTraining batch 86 Loss: 0.580552\n",
      "\tTraining batch 87 Loss: 0.502650\n",
      "\tTraining batch 88 Loss: 0.280251\n",
      "\tTraining batch 89 Loss: 0.363370\n",
      "\tTraining batch 90 Loss: 0.643375\n",
      "\tTraining batch 91 Loss: 0.282563\n",
      "\tTraining batch 92 Loss: 0.321991\n",
      "\tTraining batch 93 Loss: 0.271294\n",
      "\tTraining batch 94 Loss: 0.417591\n",
      "\tTraining batch 95 Loss: 0.267629\n",
      "\tTraining batch 96 Loss: 0.261741\n",
      "\tTraining batch 97 Loss: 0.379308\n",
      "\tTraining batch 98 Loss: 0.311484\n",
      "\tTraining batch 99 Loss: 0.474201\n",
      "\tTraining batch 100 Loss: 0.432764\n",
      "\tTraining batch 101 Loss: 0.415211\n",
      "\tTraining batch 102 Loss: 0.330662\n",
      "\tTraining batch 103 Loss: 0.469423\n",
      "\tTraining batch 104 Loss: 0.401025\n",
      "\tTraining batch 105 Loss: 0.371457\n",
      "\tTraining batch 106 Loss: 0.648746\n",
      "Training set: Average loss: 0.404231\n",
      "Validation set: Average loss: 0.385873, Accuracy: 1872/2266 (83%)\n",
      "\n",
      "Epoch: 9\n",
      "\tTraining batch 1 Loss: 0.405497\n",
      "\tTraining batch 2 Loss: 0.340432\n",
      "\tTraining batch 3 Loss: 0.373460\n",
      "\tTraining batch 4 Loss: 0.430455\n",
      "\tTraining batch 5 Loss: 0.502742\n",
      "\tTraining batch 6 Loss: 0.345399\n",
      "\tTraining batch 7 Loss: 0.318136\n",
      "\tTraining batch 8 Loss: 0.445627\n",
      "\tTraining batch 9 Loss: 0.517453\n",
      "\tTraining batch 10 Loss: 0.533036\n",
      "\tTraining batch 11 Loss: 0.386794\n",
      "\tTraining batch 12 Loss: 0.468928\n",
      "\tTraining batch 13 Loss: 0.445157\n",
      "\tTraining batch 14 Loss: 0.466139\n",
      "\tTraining batch 15 Loss: 0.380640\n",
      "\tTraining batch 16 Loss: 0.459961\n",
      "\tTraining batch 17 Loss: 0.407433\n",
      "\tTraining batch 18 Loss: 0.414221\n",
      "\tTraining batch 19 Loss: 0.357484\n",
      "\tTraining batch 20 Loss: 0.375682\n",
      "\tTraining batch 21 Loss: 0.441560\n",
      "\tTraining batch 22 Loss: 0.328247\n",
      "\tTraining batch 23 Loss: 0.332544\n",
      "\tTraining batch 24 Loss: 0.382862\n",
      "\tTraining batch 25 Loss: 0.439513\n",
      "\tTraining batch 26 Loss: 0.507340\n",
      "\tTraining batch 27 Loss: 0.322907\n",
      "\tTraining batch 28 Loss: 0.252761\n",
      "\tTraining batch 29 Loss: 0.273560\n",
      "\tTraining batch 30 Loss: 0.360570\n",
      "\tTraining batch 31 Loss: 0.506884\n",
      "\tTraining batch 32 Loss: 0.392078\n",
      "\tTraining batch 33 Loss: 0.404833\n",
      "\tTraining batch 34 Loss: 0.377296\n",
      "\tTraining batch 35 Loss: 0.449720\n",
      "\tTraining batch 36 Loss: 0.385288\n",
      "\tTraining batch 37 Loss: 0.415094\n",
      "\tTraining batch 38 Loss: 0.421994\n",
      "\tTraining batch 39 Loss: 0.291490\n",
      "\tTraining batch 40 Loss: 0.342545\n",
      "\tTraining batch 41 Loss: 0.439824\n",
      "\tTraining batch 42 Loss: 0.392726\n",
      "\tTraining batch 43 Loss: 0.323481\n",
      "\tTraining batch 44 Loss: 0.342280\n",
      "\tTraining batch 45 Loss: 0.433598\n",
      "\tTraining batch 46 Loss: 0.319355\n",
      "\tTraining batch 47 Loss: 0.340526\n",
      "\tTraining batch 48 Loss: 0.363198\n",
      "\tTraining batch 49 Loss: 0.438234\n",
      "\tTraining batch 50 Loss: 0.340505\n",
      "\tTraining batch 51 Loss: 0.412804\n",
      "\tTraining batch 52 Loss: 0.243007\n",
      "\tTraining batch 53 Loss: 0.446902\n",
      "\tTraining batch 54 Loss: 0.339466\n",
      "\tTraining batch 55 Loss: 0.396972\n",
      "\tTraining batch 56 Loss: 0.469938\n",
      "\tTraining batch 57 Loss: 0.548513\n",
      "\tTraining batch 58 Loss: 0.528016\n",
      "\tTraining batch 59 Loss: 0.474539\n",
      "\tTraining batch 60 Loss: 0.282457\n",
      "\tTraining batch 61 Loss: 0.413246\n",
      "\tTraining batch 62 Loss: 0.629373\n",
      "\tTraining batch 63 Loss: 0.407974\n",
      "\tTraining batch 64 Loss: 0.458282\n",
      "\tTraining batch 65 Loss: 0.393631\n",
      "\tTraining batch 66 Loss: 0.415371\n",
      "\tTraining batch 67 Loss: 0.376574\n",
      "\tTraining batch 68 Loss: 0.447937\n",
      "\tTraining batch 69 Loss: 0.384787\n",
      "\tTraining batch 70 Loss: 0.397053\n",
      "\tTraining batch 71 Loss: 0.322867\n",
      "\tTraining batch 72 Loss: 0.493530\n",
      "\tTraining batch 73 Loss: 0.420233\n",
      "\tTraining batch 74 Loss: 0.383111\n",
      "\tTraining batch 75 Loss: 0.387315\n",
      "\tTraining batch 76 Loss: 0.402575\n",
      "\tTraining batch 77 Loss: 0.281559\n",
      "\tTraining batch 78 Loss: 0.385554\n",
      "\tTraining batch 79 Loss: 0.348717\n",
      "\tTraining batch 80 Loss: 0.242578\n",
      "\tTraining batch 81 Loss: 0.485221\n",
      "\tTraining batch 82 Loss: 0.192280\n",
      "\tTraining batch 83 Loss: 0.174421\n",
      "\tTraining batch 84 Loss: 0.252560\n",
      "\tTraining batch 85 Loss: 0.467155\n",
      "\tTraining batch 86 Loss: 0.357641\n",
      "\tTraining batch 87 Loss: 0.471038\n",
      "\tTraining batch 88 Loss: 0.503641\n",
      "\tTraining batch 89 Loss: 0.266737\n",
      "\tTraining batch 90 Loss: 0.733258\n",
      "\tTraining batch 91 Loss: 0.320984\n",
      "\tTraining batch 92 Loss: 0.405301\n",
      "\tTraining batch 93 Loss: 0.272856\n",
      "\tTraining batch 94 Loss: 0.433413\n",
      "\tTraining batch 95 Loss: 0.395108\n",
      "\tTraining batch 96 Loss: 0.276036\n",
      "\tTraining batch 97 Loss: 0.313870\n",
      "\tTraining batch 98 Loss: 0.312968\n",
      "\tTraining batch 99 Loss: 0.495365\n",
      "\tTraining batch 100 Loss: 0.457200\n",
      "\tTraining batch 101 Loss: 0.439133\n",
      "\tTraining batch 102 Loss: 0.258515\n",
      "\tTraining batch 103 Loss: 0.394412\n",
      "\tTraining batch 104 Loss: 0.426841\n",
      "\tTraining batch 105 Loss: 0.356605\n",
      "\tTraining batch 106 Loss: 0.428323\n",
      "Training set: Average loss: 0.393993\n",
      "Validation set: Average loss: 0.301358, Accuracy: 1985/2266 (88%)\n",
      "\n",
      "Epoch: 10\n",
      "\tTraining batch 1 Loss: 0.220067\n",
      "\tTraining batch 2 Loss: 0.278645\n",
      "\tTraining batch 3 Loss: 0.296382\n",
      "\tTraining batch 4 Loss: 0.258976\n",
      "\tTraining batch 5 Loss: 0.548946\n",
      "\tTraining batch 6 Loss: 0.288598\n",
      "\tTraining batch 7 Loss: 0.248351\n",
      "\tTraining batch 8 Loss: 0.353254\n",
      "\tTraining batch 9 Loss: 0.550657\n",
      "\tTraining batch 10 Loss: 0.424343\n",
      "\tTraining batch 11 Loss: 0.416312\n",
      "\tTraining batch 12 Loss: 0.257481\n",
      "\tTraining batch 13 Loss: 0.343221\n",
      "\tTraining batch 14 Loss: 0.424199\n",
      "\tTraining batch 15 Loss: 0.365745\n",
      "\tTraining batch 16 Loss: 0.375172\n",
      "\tTraining batch 17 Loss: 0.356703\n",
      "\tTraining batch 18 Loss: 0.456918\n",
      "\tTraining batch 19 Loss: 0.509425\n",
      "\tTraining batch 20 Loss: 0.399317\n",
      "\tTraining batch 21 Loss: 0.327413\n",
      "\tTraining batch 22 Loss: 0.450219\n",
      "\tTraining batch 23 Loss: 0.411352\n",
      "\tTraining batch 24 Loss: 0.471935\n",
      "\tTraining batch 25 Loss: 0.372329\n",
      "\tTraining batch 26 Loss: 0.464414\n",
      "\tTraining batch 27 Loss: 0.371886\n",
      "\tTraining batch 28 Loss: 0.686941\n",
      "\tTraining batch 29 Loss: 0.326116\n",
      "\tTraining batch 30 Loss: 0.326285\n",
      "\tTraining batch 31 Loss: 0.425503\n",
      "\tTraining batch 32 Loss: 0.296423\n",
      "\tTraining batch 33 Loss: 0.394714\n",
      "\tTraining batch 34 Loss: 0.493666\n",
      "\tTraining batch 35 Loss: 0.400062\n",
      "\tTraining batch 36 Loss: 0.254784\n",
      "\tTraining batch 37 Loss: 0.339130\n",
      "\tTraining batch 38 Loss: 0.556117\n",
      "\tTraining batch 39 Loss: 0.325997\n",
      "\tTraining batch 40 Loss: 0.362490\n",
      "\tTraining batch 41 Loss: 0.361601\n",
      "\tTraining batch 42 Loss: 0.359411\n",
      "\tTraining batch 43 Loss: 0.386781\n",
      "\tTraining batch 44 Loss: 0.284287\n",
      "\tTraining batch 45 Loss: 0.493050\n",
      "\tTraining batch 46 Loss: 0.287337\n",
      "\tTraining batch 47 Loss: 0.294491\n",
      "\tTraining batch 48 Loss: 0.355342\n",
      "\tTraining batch 49 Loss: 0.313335\n",
      "\tTraining batch 50 Loss: 0.264267\n",
      "\tTraining batch 51 Loss: 0.284477\n",
      "\tTraining batch 52 Loss: 0.230242\n",
      "\tTraining batch 53 Loss: 0.310227\n",
      "\tTraining batch 54 Loss: 0.344332\n",
      "\tTraining batch 55 Loss: 0.243993\n",
      "\tTraining batch 56 Loss: 0.354428\n",
      "\tTraining batch 57 Loss: 0.457155\n",
      "\tTraining batch 58 Loss: 0.519132\n",
      "\tTraining batch 59 Loss: 0.402456\n",
      "\tTraining batch 60 Loss: 0.301244\n",
      "\tTraining batch 61 Loss: 0.390168\n",
      "\tTraining batch 62 Loss: 0.288465\n",
      "\tTraining batch 63 Loss: 0.311582\n",
      "\tTraining batch 64 Loss: 0.436580\n",
      "\tTraining batch 65 Loss: 0.393471\n",
      "\tTraining batch 66 Loss: 0.324425\n",
      "\tTraining batch 67 Loss: 0.292810\n",
      "\tTraining batch 68 Loss: 0.418475\n",
      "\tTraining batch 69 Loss: 0.401708\n",
      "\tTraining batch 70 Loss: 0.337061\n",
      "\tTraining batch 71 Loss: 0.315524\n",
      "\tTraining batch 72 Loss: 0.555499\n",
      "\tTraining batch 73 Loss: 0.483806\n",
      "\tTraining batch 74 Loss: 0.414734\n",
      "\tTraining batch 75 Loss: 0.460699\n",
      "\tTraining batch 76 Loss: 0.294698\n",
      "\tTraining batch 77 Loss: 0.389775\n",
      "\tTraining batch 78 Loss: 0.431773\n",
      "\tTraining batch 79 Loss: 0.361434\n",
      "\tTraining batch 80 Loss: 0.318708\n",
      "\tTraining batch 81 Loss: 0.398662\n",
      "\tTraining batch 82 Loss: 0.379507\n",
      "\tTraining batch 83 Loss: 0.291720\n",
      "\tTraining batch 84 Loss: 0.250313\n",
      "\tTraining batch 85 Loss: 0.370985\n",
      "\tTraining batch 86 Loss: 0.366936\n",
      "\tTraining batch 87 Loss: 0.308518\n",
      "\tTraining batch 88 Loss: 0.260275\n",
      "\tTraining batch 89 Loss: 0.413406\n",
      "\tTraining batch 90 Loss: 0.482120\n",
      "\tTraining batch 91 Loss: 0.277477\n",
      "\tTraining batch 92 Loss: 0.329028\n",
      "\tTraining batch 93 Loss: 0.350120\n",
      "\tTraining batch 94 Loss: 0.337907\n",
      "\tTraining batch 95 Loss: 0.308839\n",
      "\tTraining batch 96 Loss: 0.288289\n",
      "\tTraining batch 97 Loss: 0.178026\n",
      "\tTraining batch 98 Loss: 0.249288\n",
      "\tTraining batch 99 Loss: 0.395337\n",
      "\tTraining batch 100 Loss: 0.339850\n",
      "\tTraining batch 101 Loss: 0.461433\n",
      "\tTraining batch 102 Loss: 0.389474\n",
      "\tTraining batch 103 Loss: 0.477621\n",
      "\tTraining batch 104 Loss: 0.401677\n",
      "\tTraining batch 105 Loss: 0.375732\n",
      "\tTraining batch 106 Loss: 0.365571\n",
      "Training set: Average loss: 0.367430\n",
      "Validation set: Average loss: 0.296165, Accuracy: 1990/2266 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_criteria = nn.CrossEntropyLoss()\n",
    "epoch_nums = []\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "epochs = 10\n",
    "print('Training on', device)\n",
    "for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss = test(model, device, test_loader)\n",
    "        epoch_nums.append(epoch)\n",
    "        training_loss.append(train_loss)\n",
    "        validation_loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00a8802",
   "metadata": {},
   "source": [
    "**Graph Between Training And Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceaf7dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epoch_nums, training_loss)\n",
    "plt.plot(epoch_nums, validation_loss)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['training', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22719f92",
   "metadata": {},
   "source": [
    "**Evaluate Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00e9f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting predictions from test set...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAFTCAYAAABWG+UsAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA14ElEQVR4nO3dd7wU1fnH8c+XEsAAAiJdRSwxStSfoNEYW+wVG2pi14QkmkQTNWrsMSiWGE2MBY2CWAF7F1GwxIi9IdhApQmCICoi5fn9cc7FYb1ld7lzZ+/wvO9rXrt7pj2ze/fZM2dmzsjMcM45V7wmWQfgnHONjSdO55wrkSdO55wrkSdO55wrkSdO55wrkSdO55wrUaNInJJaSbpf0jxJI1ZgOYdKeqw+Y8uCpIclHZni8reW9K6kLyTtW+K8uXiPK5mkIZL+lnUcK0rS9pKmZB1HOeo1cUr6haQX4xduevyC/7QeFn0g0BlYzcz6l7sQM7vFzHaph3iWE/8BTNJdBeWbxPIxRS7nXEk31zWdme1uZkPLjLWHpFskzZb0paRxkvYqmOyvwJVm1trM7qlmGT+V9N/4QzZH0rOSNo+xpfIeNxRJYyR9LWmNRNlOkiYXOX9Rn2FaJB0V/+dOKSifImn7IubvGedvllaMeVBviVPSn4DLgQsISW5N4CqgXz0sfi3gHTNbXA/LSsss4CeSVkuUHQm8U18rUFD2ZyapA/AM8A2wEdAR+Adwq6QDE5OuBbxVwzLaAg8A/wI6AN2B84CF5cZVgb4EzspixfWUsOYAp8bPqiI1+sRsZis8AKsCXwD9a5mmBSGxTovD5UCLOG57YApwEjATmA4cHcedR/iiL4rrOBY4F7g5seyegAHN4uujgA+A+cAk4NBE+TOJ+X4CvADMi48/SYwbA5wPPBuX8xjQsYZtq4r/GuD4WNY0lp0NjElMewXwMfA58BKwTSzfrWA7X0vEMTDGsQBYN5b9Mo6/GhiZWP5FwGhA1cR5PvAm0KSg/FTgQ0DA+8DSuK4vqj6jxLR9gbm1fM6F73G12xvHnQuMAG6O7/EbwPrA6fH/4GNgl4L/s//E/4+pwN+ApnHcusDY+Fl+CtyRmG8DYBQhoUwEDqol/jHAOTGedWPZTsDkxDTdgDsJP5aTgD/U8RlOBnYq2O6bC/53jwU+Ap6K5SOAGXF7ngI2Ssw/BPhbbe8/cD9wTqJ8CrB9fN4EOC1+1rOB4UCHOO6jGM8Xcdgq/m/0ieMPi+M3jK9/CdxTwnf81Lhdw6rKEjH+ARgP9KiPvJTmUF81zq2AlsDdtUxzBrAlsCmwCbAFcGZifBfCF6M74Z/o35Lam9k5hFrsHRZ2Hf9TWyCSvg/8E9jdzNoQkuOr1UzXAXgwTrsacBnwYEGN8RfA0UAn4HvAybWtG7gJOCI+35VQa5tWMM0LhPegA3ArMEJSSzN7pGA7N0nMczgwAGhD+CdOOgnYOO6ibUN47460+J9YYGfgTjNbWlA+nLCHsL6ZrUP48uwd4yisSb4DLJE0VNLuktrX+G7Usr2J8XsTvkTtgVeARwlf7O6EJoNrE9MOBRYTkuT/AbsQvrgQfhQei8vpQagRV/0/jIrr7gT8HLhK0ka1xDwVuI6Q4JYTa/z3A6/FGHcETpS0ax2fYV22A35I+L8BeBhYL8b8MnBLCcuCUGP+Y/w/L/QHYN+4zm7AZ8C/47ht42O7uA3PEX6Qtk+M/yDOW/V6bHxezHe8A2GPZkAyIElnEZL+dmZW8e2e9ZU4VwM+tdp3pQ8F/mpmM81sFqEmeXhi/KI4fpGZPUT4tftBmfEsBXpLamVm082sut3OPYF3zWyYmS02s9uACYQvcpUbzewdM1tASC6b1rZSM/sv0EHSDwgJ9KZqprnZzGbHdf6d8Ctd13YOMbO34jyLCpb3FaEWcBmh5vb7Wv7xOhJqa4WmJ8bXysw+B35KqHVcB8ySdJ+kzjVMX9f2Pm1mj8b/nRHA6sCguJ23Az0ltYvL3x040cy+NLOZhGaGQ+JyFhG+kN3M7GszeyaW70WoLd4YY3iZUFtMNk1U50Jg72oS7ObA6mb2VzP7xsw+iO/DId9ZQmnOjdu1AMDMbjCz+fGH61xgE0mrFrswM3uV8ENyajWjfw2cYWZTEss/sJbd57F8myi3Ibw3Va+349vEWdd3fCmhFrywajsJLVCXEX4wdojzVbz6SpyzgY51tFt0Y/na0oexbNkyChLvV0DrUgMxsy+Bg4HfANMlPShpgyLiqYqpe+L1jDLiGQb8DtiBamrgkk6S9HY8sDKXUMuuK2F9XNtIMxtHqAWIkOBr8inQtZryronxdTKzt83sKDPrAfQmvJeXVzdtEdv7SeL5AsIP8JLEawjv+1pAc8JnOjcu61pCjQzgz4TtHyfpLUnHxPK1gB9XzRPnO5RQ+6ltG2cBVxJqvUlrAd0KlvcXQrv+ilj2GUtqKmmQpPclfU7Y1YciftgKnA38VlLhtq4F3J2I/21gCTVvw1hgm7icpsAdwNaSehI+z1fjdHV9x2eZ2dcFy25HqH1eaGbzit6yjNVX4nwO+JpQ/a/JNMIHVmVNvrsbW6wvgVUSr5f7x4g1mJ0JCWECoUZQVzxVMU0tM6Yqw4DjgIdibXCZuCt9KnAQ0N7M2hHasFQVeg3LrLULK0nHE2py0wgJpCaPAwdUc4DpIMIXt+QDWWY2gdDm1ruauOra3lJ8TDgA1dHM2sWhrZltFOOYYWa/MrNuhBrVVZLWjfONTcxTtQv62yLWeQnhB7BPQRyTCpbXxsz2iOOr+6xq/X+tZr5fEA6q7kRITD1jeUnvW/xs7iIk9qSPCU1ZyW1oaWZTq4vfzN4jVBz+QGiDnU+oVAwgtGdXNf3U9R2v7r35jLBXcKOkrUvZvizVS+KMvxRnE9ol95W0iqTmsQ3s4jjZbcCZklaX1DFOX+5pG68C20paM+6+nF41QlJnSfvEtq2FhF3+JdUs4yFgfYVTqJpJOhjYkHDEuGxmNomw+3JGNaPbENroZgHNJJ0NJI98fkLYNS36c5G0PuEgyWGE3aI/S9q0hsn/Edf3H0ldJLWU9PMY6yk1tIsWrm+DWIvsEV+vQWg3/F81k9e1vUUzs+mEXc+/S2orqYmkdSRtF+PoXxUT4ctohM/9AcLnfHj8n2wuaXNJPyxinXOBv7P8j9E44HNJpyqcX9xUUm/F07Go/jN8FTgkrrsvdTcTtCH8784mJNwL6oq1FucR2unbJcquAQZKWgsgfierzn6ZRdil7lWwnLGEPamq3fIxBa+hzO+4mY0h7AXcLenHxW5YlurtdCQzuwz4E6ExeBbhV+13wD1xkr8BLwKvE46evhzLylnXKMLuwuuEI7XJZNeEcMBkGuEo6naEGmDhMmYTfulOIvyD/hnYy8yK2l2tI75nzKy62vSjhEb/dwi7MV+z/G541cn9syW9XNd6YtPIzcBFZvaamb1LqF0Mk9SimrhmE9onWxKOXs4mfGaHm9kdRW7efODHwPOSviQkzDcJ72Ohura3VEcQDtKNJyTHkXzbzLB5jOkL4D7gBDObFGtHuxDaIKcRakoXEWroxbiCxA9vbEbYm9DePYnQvHE9oWYI1X+GZwHrxJjPIxyoqs1NhPdratzW6n6UihJ/yIcB308UX0F4jx6TND8u/8dx+q+IZ3HEXfkt4zxjCQn9qRpewwp8x+N3+mjgPkl96po+ayqikuGccy6hUVxy6ZxzlcQTp3POlcgTp3POlcgTp3POlcgTp3OuUZJ0g6SZkt5MlHWQNEqhW8RRyUuCJZ0u6T1JEyXtmijvI+mNOO6fkuo8X7bRHVVv1eeExhWwW85nz1+RdQiuTC2blXXhAq3+73dlfWcXvHJlreuTtC3hPO2bzKx3LLsYmGNmgySdRrjw4lRJGxLOM92CcDXT44S+GZZIGgecQDgt6yHgn2b2cG3r9hqncy5dalLeUAcze4pwrnZSP0JnMMTHfRPlt8fr5CcB7wFbSOoKtDWz5+IFIDdR+xWQgCdO51zapLIGSQMUOkavGgbUvTI6x6vMqq42q+rLoDvLX3wxJZZ1j88Ly2vVuDsTdc5VvjL73jazwcDg+oqiulXUUl4rr3E659JVZo2zTJ/E3W/i48xYPgVYIzFdD8IluFPi88LyWnnidM6lK6U2zhrcR7hlDfHx3kT5IZJaSFqb0En0uLg7P1/SlvFo+hGJeWrku+rOuXSVX3usY7G6jdAzfUeFu2WeAwwChkuquhVJfwAze0vScEKnKYsJt7ip6rzlt4SuEVsROqWp9Yg6eOJ0zjVSZvbzGkbtWMP0Awk9PxWWv0g1/cnWxhOncy5d5e92VyxPnM65dKW0q54lT5zOuXR5jdM550rkNU7nnCuR1zidc65EXuN0zrkSeY3TOedK5InTOedK1MR31Z1zrjRe43TOuRL5wSHnnCuR1zidc65EXuN0zrkSeY3TOedK5DVO55wrUQ5rnPnbIuecS5nXOJ1z6fJddeecK1EOd9U9cTrn0uU1TuecK5HXOJ1zrkSeOJ1zrkS+q+6ccyXyGqdzzpXIa5zOOVcir3E651yJvMbpnHOlkSdO55wrjSdO55wrVf7ypidO51y6vMbpnHMl8sTpnHMlymPizN8JVs45lzKvcTrnUpXHGqcnTudcuvKXN7PZVZd0bDVlg7KIxTmXLkllDZUsqxrngZK+NrNbACRdBbTIKBbnXIoqPQmWI6vEuT9wn6SlwO7AHDM7LqNYnHMp8sS5giR1SLz8JXAP8CzwV0kdzGxOQ8bjnEufJ84V9xJghObiqsc942BArwaOxzmXtvzlzYZNnGa2dkOuzzmXvTzWOLM6qt5fUpv4/ExJd0n6vyxicc6lK49H1bO6cugsM5sv6afArsBQ4JqMYnHOpcgTZ/1ZEh/3BK42s3uB72UUi3MuTSpzqGBZnY40VdK1wE7ARZJa4NfNO5dLlV57LEdWyeog4FFgNzObC3QATskoFudcivK4q55JjdPMvgLuktRJ0pqxeEIWsTjn0lXpSbAcmSROSfsAfwe6ATOBNQmJc6Ms4nHOpSePiTOrXfXzgS2Bd+K5nTsRriByzuVNDg8OZZU4F5nZbKCJpCZm9iSwaUaxOOdSlFYbp6Q/SnpL0puSbpPUUlIHSaMkvRsf2yemP13Se5ImStp1RbYpq8Q5V1Jr4CngFklXAIszisU518hI6g78AehrZr2BpsAhwGnAaDNbDxgdXyNpwzh+I2A34CpJTctdf1aJsx+wAPgj8AjwPrB3RrE451KU4lH1ZkArSc2AVYBphNwyNI4fCuwbn/cDbjezhWY2CXgP2KLcbcrqqPqXAJLaAvdnEYNzrmGkcXDIzKZKuhT4iFAJe8zMHpPU2cymx2mmS+oUZ+kO/C+xiCmxrCxZXav+a0mfAK8DLxJ6TXoxi1iccykr8+CQpAGSXkwMA5YtMrRd9gPWJpyd831Jh9URRSErd5OyunLoZGAjM/s0o/U3mGvO/jm7b7MRs+Z8Qd+Dw91B2rddhWEXHsVa3Trw4bQ5HHbajcydvwCAk4/eiaP6bcmSJUs56dK7ePy5cHrr/23Qg8HnHUqrFs159NnxnHTJXZltkwtuGTaUO0eOwMw44MD+HHbEUUycMIG//fUcvvrqK7p1686FF19K69atsw41U+XWOM1sMDC4htE7AZPMbFZcx13AT4BPJHWNtc2uhNMdIdQw10jM34Owa1+WrNo43we+ymjdDWrY/ePo9/vl+y85+aidGPPCO/xov78x5oV3OPmonQDYYO3O9N9lMzbrfyH7/P4arjitP02ahH+6f55+EL/72x303vdvrLPG6uzykx82+La4b7377jvcOXIEt9w+ghF33ctTY8fw4YeTOe/sMzjhjydx5z3387OddmLIDddnHWrmUmrj/AjYUtIqChPvCLwN3AccGac5Erg3Pr8POERSC0lrA+sB48rdpqwS5+nAfyVdK+mfVUNGsaTq2VfeZ8685X8j9tquNzc/ED6zmx8Yx97b/yiUb/8jRjz2Mt8sWsKH0+bw/sez2HyjtejSsS1tWrfk+TcmA3Drgy8sm8dlY9IH77PxJpvQqlUrmjVrRp++m/PE46OYPHkSffpuDsBWW23N6FGPZRxp9tJInGb2PDASeBl4g5DLBgODgJ0lvQvsHF9jZm8Bw4HxhAPSx5vZkmoWXZSsdtWvBZ4gbPDSjGLITKfV2jDj088BmPHp56zeoQ0A3Vdfleff+HDZdFM/mUe3TquyaPESpn4yN1E+l26d2jVkyK7Auuuuz7+uuJy5cz+jRYuWPPP0U2y4UW/WXW99xjw5mh1+thOPPfoIM2ZMzzrUzKV15ZCZnQOcU1C8kFD7rG76gcDA+lh3VolzsZn9KaN1V65q/sHMqv/HMyu7XdvVg17rrMPRx/6SX//yGFZZZRXW/8EPaNa0KeedP5BBFw7k2quvYvsdfkbz5t5bYqVfBVSOrBLnk/EI2f2EXwgAarpZW5x2AECzNX9Gs469GyTItMycPZ8uHdsy49PP6dKxLbPmzAdg6sy59OjSbtl03TuvyvRZ85g6cy7dOyfL2zF91rwGjtoV2v+A/ux/QH8A/nn5ZXTu3Jm1e63DtdfdAMDkyZN4auyYDCOsDH6tev35BbGdk3AqUq2nI5nZYDPra2Z9G3vSBHjwqTc5bK9w7u1he23BA2PfDOVj36T/LpvxveZNWatbB9ZdY3VeeOtDZnz6OV98uZAteq8FwC/23HzZPC47s2fPBmD6tGmMfvwxdt9jr2VlS5cu5bprr6b/wYdkGWJFSPEE+MxkdQJ8rTdtk7SzmY1qqHjSNHTgEWzTd106tmvNew+dx/nXPsylQx7n5kFHc2S/Lfl4xmcceuqNALz9wQzuHPUKr4z8C4sXL+HEi0aydGnYJf/DhcMZfO6htGrZnMeeHc+jz47PcrMccNKJv2fe3Lk0a9aMv5x5Dm1XXZVbhg3l9ttuBWDHnXZm3/0OyDjK7FV4DiyLKrGtTNLLZrZZdeNa9Tmh8gJ2Rfvs+SuyDsGVqWWz8lor1zvlkbK+s+9eslvFptys2jjrUrFvmHOuNHmscVZq4vRapXM5UentleWo1MTpnMuJHObNik2ck7MOwDlXP6ouG86TzBKnpJ8APZMxmNlN8XH/jMJyzrk6ZXWztmHAOsCrQNX1ogbclEU8zrn0+K56/ekLbGiVeC6Uc65e+cGh+vMm0AXwHhCcy7kc5s2GTZyS7ifskrcBxksax/LXqu/TkPE459LnNc4Vd2kDr885lzFPnCvIzMYCSLrIzE5NjpN0ETC2IeNxzqUvh3kzs96Rdq6mbPcGj8I5lzrvHWkFSfotcBzQS9LriVFtgGcbMhbnXMOo8BxYloZu47wVeBi4EDgtUT6/pk6MnXONW6XXHsvR0InTzGyypOMLR0jq4MnTufzJYd7MpMa5F6HHd2P57uMM6NXA8TjnUuY1zhVkZnvFp88ATwFPm9mEhozBOdewcpg3MzuqfiPQFfiXpPcljZR0QkaxOOdS5EfV64mZPSFpLLA5sAPwG6A34PdVcC5nKjwHliWr3pFGA98HngOeBjY3s5lZxOKcS1el1x7LkdWu+uvAN4Ra5sZAb0mtMorFOZciqbyhkmW1q/5HAEmtgaMJbZ5dgBZZxOOcc6XIalf9d8A2QB/gQ+AGwi67cy5n8rirXlLiVHgHegBrAK+Z2ZdlrrcVcBnwkpktLnMZzrlGIId5s/g2TknHAVMJNcSngR/E8rsknVjKSs3sEjN73pOmc/mXx9ORikqckk4h1BCvA37G8lf8jAEOrvfInHO5kMfEWeyu+vHA2WZ2saSmBeMmAuvXb1jOubyo8BxYlmITZxfC9eXVWQq0rJ9wnHN5U+m1x3IU28b5HrBdDeO2BcbXTzjOubxZmc/jvBy4StI3wMhY1knSscCfgF+lEJtzLgfyWOMsKnGa2fWS2gNnA+fF4oeAr4BzzezWlOJzzjVyOcybxZ/HaWaXSLoG+AmwGjAHeM7M5qUVnHOu8WuSw8xZ0gnwZjYfeDSlWJxzOZTDvFl84pTUCTgR2ILQl+Z04Hngn2b2SSrROecavTy2cRZ7AvzWwLvAr4FPgdHx8TfAu3G8c859RxOVN1SyYmucVxLO49w7eX167N3oAeBfwGb1H55zrrHLY42z2MS5AXBgYaceZvaFpEuBEfUemXMuF3KYN4tOnOMJVw9VpyvgN1xzzlVL5C9zFps4fw8Mk/QFcI+ZLZTUAtgPOA04Iq0AnXOu0hSbOO8FViHcF52YQFvHcV8DdyfbMcysUz3G6JxrxCr9QE85ik2c/wYszUCcc/m00h4cMrNzU47DOZdTOcyb2dxzyDm38lipL7mUtBVwLKHT4u/0v2lmW9RjXM65nMhh3iz6yqGdgacIN2r7KTAL+ALYhNDhx5tpBeica9zyeOuMYjsy/itwBbBnfH2Wmf2MUPtcRLjvkHPOfUceOzIuNnFuCDxMuE2GAd8HMLMPgXOBM9IIzjnX+DWRyhqKIamdpJGSJkh6W9JWkjpIGiXp3fjYPjH96ZLekzRR0q5lb1OR030NNDEzI/SKtE5i3OeEXXjnnPsOlTkU6QrgETPbgNB0+DbhopzRZrYeoUOi0wAkbQgcAmwE7Ea4q0XhzSeLUmzifI14H/UYyOmSdpa0HWE3/o1yVu6cy7+02jgltSXc8+w/AGb2jZnNBfoBQ+NkQ4F94/N+wO1mttDMJhHupVbWQe1iE+flfHsC/F+ALwkdGj8JdCLcPtg5574jxW7lehEOVN8o6RVJ10v6PtDZzKYDxMeqKxm7Ax8n5p8Sy0pW7AnwDyWeT5XUB1gXaAVMMLNvylm5cy7/yj1CLmkAMCBRNNjMBideNyN0Z/l7M3te0hXE3fKaFllNWVlXRJZ1Anxs63y3nHmdcyuXco+QxyQ5uJZJpgBTzOz5+HokIXF+IqmrmU2X1BWYmZh+jcT8PYBp5cRWygnw3YC94soKT4A3Mzu1nACcc/mW1jmZZjZD0seSfmBmE4EdCV1gjgeOBAbFx3vjLPcBt0q6DOgGrAeMK2fdRSVOSfsBtwFNCdm7cNfcAE+czrnvSLl3pN8Dt0j6HvABcDTh2M1wSccCHwH9AczsLUnDCYl1MXC8mS0pZ6XF1jgvAB4DjjKzOeWsyDm3ckrzKiAzexXoW82oHWuYfiAwcEXXW2ziXIPQAOtJ0zm30iv2dKT/8u15nM45V7SUT4DPRI01TkmrJF7+idCO8AUwCphbOL2ZfVXv0TnnGr2VrVu5L1j+HCcBN1LzeU9lXbrknMu3HObNWhPnMfjtMpxzK6jSu4grR42J08yGNGAczrmcymHeLO/KIUk/AjYAPgGeMbOl9RqVcy438tjGWeNRdUnHSBpRTfktwKvAHYROPsZJapdWgM65xm1l68j4CGBGskDSL4GfA0OAjQln5K8J/Dml+JxzjVweb51R2676BoT7qScdTkimA+KlSm9KWpPQg8lf0glxeR+N/XtDrMalpP3mv8s6BFemBa9cWdZ8xZ4s3pjUljjb8m2vIkhqAWwJ3FZwfecrhFqnc859R6XXHstR24/BR4Qu5qtsCzQntGsmrQIsqOe4nHM5kWJHxpmprcY5AjhL0gzC0fMLCSfF31cw3U8IXdA759x3VHoSLEdtifNCYHNC56AQbpfxKzP7rGoCSS0JJ8pfl1qEzrlGLY+76rWdAP8VsJukdYF2wEQzm1/N/PsA76cWoXOuUVvZapwAmFmNu+Fm9gXwUr1G5JzLlRxWOMu7csg554qVxyuHPHE651KVx/M487hNzjmXKq9xOudSlcM99dISp8J5BT0I9yB6zcy+TCUq51xu5LGNs+hddUnHAVOBD4GnifcgknSXpBNTic451+itbL0jLSPpFOAywonuP2P5eymNAQ6u98icc7mwsl1ymXQ8cLaZXSyp8N5CE4H16zcs51xe5HFXvdjE2YWaT3RfCrSsn3Ccc3mTw7xZdBvne8B2NYzbFhhfP+E45/JmZd5Vvxy4StI3fNvpRydJxxLuuf6rFGJzzuWAqPAsWIaiEqeZXS+pPXA2cF4sfgj4CjjXzG5NKT7nXCNX6bXHchR9HqeZXSLpGkL/m6sBc4DnzGxeWsE55xq/lTpxAsRu5R5NKRbnXA6tVP1xJsWT32tlZleteDjOubxZmWuctd3ezuKjJ07n3HfksMJZ3OlIZtakcAA6EO6x/hqwYZpBOucaryZSWUMlK7t3JDObC9whaVXgWmD7eorJOZcjK/Ouem0mAX3rYTnOuRyq8MpjWVaoI2NJXYGTCMnTOedWCsUeVZ/FtweBqnwPaAN8Dexfz3E553Kiycp65RDVH1X/GpgCPGJms+svJOdcnuRxV73OxCmpOfA4MMnMpqUfknMuT/J4cKiYNs4lwBPAD1OOxTmXQyvl6UhmtlTSu0DnBojHOZczFZ4Dy1JsG+cZwEWS3jCzN9IMyDmXL5VeeyxHjYlT0rbAy2b2BXAmoUekVyVNBT6h4Ci7mW2RZqDOucYph3mz1hrnk8BWwDjgzTg451xJVuhk8QpVW+Jc9jthZkenHYikFma2MO31OOcaVh67lcvkx0DSDQWvWxN6lHfO5YzKHCpZXQeH9pC0QTELMrObSljvVElXm9lv4y05HiTcs905lzMr1cGh6Owil2NA0YnTzM6SdFG8FUcfYJCZ3Vns/M65xiN/abPuxLkD8GJ9rUxS8pr2ccBZ8dEk7W9md9XXupxzlSGHFc46E+cCM/uyHte3d8HrV4DmsdwAT5zO5UweDw7VR3+cRWuIo/POucqSx9ORsjqqfrGktpKaSxot6VNJh2URi3MuXZLKGopcdlNJr0h6IL7uIGmUpHfjY/vEtKdLek/SREm7rsg21Zg4472Fxq3Iwmuxi5l9DuxF6JpufeCUlNblnMuvE4C3E69PA0ab2XrA6PgaSRsChwAbAbsBV0lqWu5Ks6pFN4+PewC3mdmcjOJwzqUsrfM4JfUA9gSuTxT3A4bG50OBfRPlt5vZQjObBLwHlH2ZeIO2cSbcL2kCsAA4TtLqhI6RnXM5k+LBocuBPxPuRFGls5lNBzCz6ZI6xfLuwP8S002JZWXJpMZpZqcRroPva2aLgC8JvwjOuZxpUuYgaYCkFxPDgKplStoLmGlmLxUZRnXZu/B2QEXLqsYJIdvvLKlloqyUq4+cc41AuTVOMxsMDK5h9NbAPpL2AFoCbSXdDHwiqWusbXYFZsbppwBrJObvAZR9R4usjqqfA/wrDjsAFwP7ZBGLcy5dabRxmtnpZtbDzHoSDvo8YWaHAfcBR8bJjgTujc/vAw6R1ELS2sB6hItvypJVjfNAYBPgFTM7WlJnlm/gdc7lRAOf/z4IGC7pWOAjoD+Amb0laTgwHlgMHG9mS8pdSVaJc0G8JcdiSW0J1eleGcXinEtR2rcHNrMxwJj4fDawYw3TDQQG1sc6s0qcL0pqR+gR6SXgC1ag2uycq1w5vOIym8RpZsfFp9dIegRoa2avZxGLcy5dymH/SJkdVZe0MdCzKgZJ63rvSM7lj9c460nsAX5j4C1gaSz23pGcy6G02zizkFWNc0sz2zCjdTvnGlAea5xZXav+XLzo3jmXc1J5QyXLqsY5lJA8ZwALCee7mpltnFE8zrmU+MGh+nMDcDjwBt+2cTrncqhJ/vJmZonzIzO7L6N1O+cakNc4688ESbcC9xN21QHw05Gcc41BVomzFSFh7pIo89ORnMuhSj/QU46srhyq9aZtkk43swsbKh7nXHp8V73h9AdylzgvOO9M/vv0WNp36MCw4aG3q/9c+2/uv3sk7dqHe0r9+vgT2eqn27Jo0TdcMvA8Jox/CzURJ5x8Opv1Lbunf1ema845lN237c2sOfPp2/8CANq3XYVhFx3DWt068OG0ORz25/8wd/4CAE4+ZheO6rcVS5Yu5aSLR/L4c+F2OAft1odTjtkVM2P6rHkcc+ZQZs+tzztvV648Hhyq1Dt35vCthj323pe//+va75Qf9IsjGHLbXQy57S62+um2ANx390gAbhp+D5dfdT1X/uMSli71ExAa2rD7/0e/4/+9XNnJR+/MmHET+VG/vzJm3EROPjq0OG3Qqwv9d92MzQ4cyD7HX8UVpx9EkyaiadMmXHLKgew24Aq2OPhC3nx3Kr85eLssNicTKvOvklVq4iy7S/tKtulmfWm76qpFTTv5g/fps8WWALTvsBpt2rRhwvg30wzPVePZl99nzryvlivba/uNufn+5wG4+f7n2XuHjZeVj3j0Zb5ZtJgPp83m/Y8/ZfPePZed0P39Vt8DoE3rVkyfNa9hNyRDeTwBvlITZ4W/bfXrruG3cuTB+3HBeWfy+efhC7Xu+j/g6TFPsHjxYqZNncLEt8cz85MZGUfqADqt1oYZn34OwIxPP2f1DuFeYd1XX5UpMz5bNt3UmZ/RrdOqLF68lBMuuIMXhv+FDx4byA97dWHIPf/NJPYspHWXyyxldeuMresoG9GA4WRqvwMP5o57H+HG2+5ktY6rc+U/LgFgz332p1Pnzvzy8IP4598H0XuTTWnatFKbpB1QbTXJDJo1a8KvDtyGLX9+Eb12OYM335nKKcfsUs0C8qmJVNZQybKqcf6rtjIzuyA5Inm3u5tuuC714BpSh9U60rRpU5o0acI++x3I22+9AUCzZs34w0mnMeS2uxh02ZV8MX8+PdZcM+NoHcDM2fPp0rEtAF06tmXWnPkATJ05lx5d2i+brnun9kyfNY9N1u8BwKQpnwIwctTLbLnJynPDA69xriBJW0k6CVhd0p8Sw7lA05rmM7PBZtbXzPoeccyvGizehvDprFnLnj/15OP0Wmc9AL5esIAFC0Lb2gv/+y9NmzZl7V7rZhKjW96DY9/gsL1/DMBhe/+YB8aEPrgfHPM6/XfdjO81b8Za3VZj3TVX54U3JzNt1jw26NWFju1bA7DjlhswcdJK1OySw8zZ0Pt+3wNax/UmbyL/OeEGbrl2zl9O5tUXX2Du3Lnst/vPOPbXx/PKSy/w7sQJSKJLt26c8pdzAfjsszn86XcDaKImdOzUibPOH5Rt8CupoRcexTZ91qNju9a898j5nH/NQ1x64yhuvugYjtx3Kz6e/hmH/vk/ALz9wQzufOwVXrnzDBYvWcqJg4azdGk4/eiCwQ8z6voTWbR4CR9Nn8OAc27OeMsaTqUfIS+HzBr+ALaktczsw3LmnfXF4lwecV9ZrLnNiVmH4Mq04JUry8qA4z6YV9Z3doteq1Zsxs3qaMMQSd95M83sZ1kE45xLT8VmvxWQVeI8OfG8JXAA4V7Hzrm8yWHmzOpa9ZcKip6VNDaLWJxz6cpjG2dWN2vrkHjZBOgDdMkiFudcuir8lMyyZLWr/hLhskoRdtEnAcdmFItzLkU5zJuZ7aqvncV6nXMZyGHmzGpXvTnwW2DbWDQGuNbMFmURj3POlSKrXfWrgebAVfH14bHslxnF45xLiR8cqj+bm9kmiddPSHoto1iccynK48GhrDr5WCJpnaoXknoBSzKKxTmXohxeqp5ZjfMU4ElJHxDeo7WAWu9D5JxrpCo9C5Yhq6PqoyWtB/yA8LZOMLOFdczmnGuEvI2zfvUBesYYNpGEmd2UYTzOuRTksY0zq9ORhgHrAK/ybdumAZ44ncuZHObNzGqcfYENLYs+7ZxzDSuHmTOrxPkm4dr06Rmt3znXQLyNcwVJup+wS94GGC9pHLDsoJCZ7dOQ8Tjn0udtnCvu0gZen3MuYznMmw2bOM1sLICki8zs1OQ4SRcB3ienc3mTw8yZ1ZVDO1dTtnuDR+GcS53K/KtkDd3G+VvgOKCXpNcTo9oAzzZkLM65huFtnCvuVuBh4ELgtET5fDOb08CxOOcaQA7zZoMnTjOzyZKOLxwhqYMnT+dyKIeZM4sa514sf+uMKgb0auB4nHOuZA19VH2v+PQZ4CngaTOb0JAxOOcaVqUf6ClHVkfVbwS6Av+S9L6kkZJOyCgW51yKpPKGSpZVt3JPxPuobw7sAPwG6A1ckUU8zrn0VHgOLEtWvSONBr4PPAc8TbiVxswsYnHOpSyHmTOrXfXXgW8ItcyNgd6SWmUUi3MuRX4CfD0xsz8CSGpNuGXGjYTeklpkEY9zLj2V3l5ZjkxqnJJ+J+kOQkfG+wI34JdcOpdLad2sTdIakp6U9Lakt6oOMEvqIGmUpHfjY/vEPKdLek/SREm7lrtNWfXH2Qq4DHjJzBZnFINzriGkV+NcDJxkZi9LagO8JGkUcBQw2swGSTqNcJXiqZI2BA4BNgK6AY9LWt/MSr7DbiY1TjO7xMye96TpXP6l1cZpZtPN7OX4fD7wNtAd6AcMjZMNJezVEstvN7OFZjYJeA/YopxtyurgkHNuJdEQ53FK6gn8H/A80NnMpkNIrkCnOFl34OPEbFNiWck8cTrnUlVuG6ekAZJeTAwDql1+OMh8J3CimX1eRyiFyrrvWZa3B3bOrQTKPapuZoOBwbUvW80JSfMWM7srFn8iqauZTZfUFag6R3wKsEZi9h7AtHJi8xqncy5l6RxXlyTgP8DbZnZZYtR9wJHx+ZHAvYnyQyS1kLQ2sB4wrpwt8hqncy5VKZ7HuTVwOPCGpFdj2V+AQcBwSccCHwH9AczsLUnDgfGEI/LHl3NEHTxxOudSllbeNLNnaln8jjXMMxAYuKLr9sTpnEuVXznknHPOa5zOuXRVeocd5fDE6ZxLV/7ypidO51y6cpg3PXE659KVx4NDnjidc6nyNk7nnCtV/vKmJ07nXLpymDc9cTrn0uVtnM45VyJv43TOuRLlscbpl1w651yJvMbpnEtVHmucnjidc6nyNk7nnCuR1zidc65EOcybnjidcynLYeb0o+rOOVcir3E651LlB4ecc65EfnDIOedKlMO86YnTOZeyHGZOT5zOuVR5G6dzzpUoj22cMrOsY3AJkgaY2eCs43Dl8c9v5eDncVaeAVkH4FaIf34rAU+czjlXIk+czjlXIk+clcfbxxo3//xWAn5wyDnnSuQ1TuecK5EnTuecK5Enznoi6SFJ7eJwXKJ8e0kPZBzbZEkds4whSw3x2cRl/aQ+llXieodIOrCh17uy88RZT8xsDzObC7QDjqt9ateQGuiz2R5o8MTpsuGJs0iS/izpD/H5PyQ9EZ/vKOnmRK1uELCOpFclXRJnby1ppKQJkm6Rar4ILS7nAknPSXpR0maSHpX0vqTfxGlaSxot6WVJb0jqF8u/L+lBSa9JelPSwQXLbiXpEUm/SuEtykwan02c95X4/t4gqUUsX1Z7l9RX0hhJPYHfAH+My96mhjiHSLpa0pOSPpC0XVz225KGJKa7On72b0k6L1E+SNJ4Sa9LurSa5Z8f1+Hf67SZmQ9FDMCWwIj4/GlgHNAcOAf4NTAZ6Aj0BN5MzLc9MA/oQfiheg74aS3rmQz8Nj7/B/A60AZYHZgZy5sBbePzjsB7hD5oDgCuSyxr1cQyewKPA0dk/V5W+mcDtAQ+BtaP090EnJh4LzvG532BMfH5ucDJdcQ5BLg9flb9gM+BH8V1vwRsGqfrEB+bAmOAjYEOwES+PROmXWKZBwIXA9dWjfch3cF/mYr3EtBHUhtgIeFL1hfYhvBlrc04M5tiZkuBVwlf4NrcFx/fAJ43s/lmNgv4WlI7whfvAkmvE5Jhd6BznH4nSRdJ2sbM5iWWeS9wo5ndVNTWNi71/dn8AJhkZu/EaYYC29ZTrPdbyHhvAJ+Y2Rtx3W/x7f/FQZJeBl4BNgI2JCTZr4HrJe0PfJVY5lmERPrruGyXMk+cRTKzRYTaxtHAfwlfyB2AdYC365h9YeL5Eurulapq+qUF8y6N8x5KqIH2MbNNgU+AlvGL3ofwpbxQ0tmJeZ8Fdq+tmaCxSuGzqe09Wsy335uWZYRb62craW3gZGBHM9sYeJDw2S4GtgDuBPYFHknM+wLhh6NDGfG4MnjiLM1ThH/qpwhfzt8Arxb8ys8n7FqnaVXCbvsiSTsAawFI6gZ8ZWY3A5cCmyXmORuYDVyVcmxZqc/PZgLQU9K68fXhwNj4fDLhxwlC00ipy65LW+BLYJ6kzsDuENq1CU0vDwEnApsm5nmE0H77YKx1u5R54izN00BX4Dkz+4Sw67TcrqCZzQaejQdnLqlmGfXhFqCvpBcJtc8JsfxHwDhJrwJnAH8rmO9EoKWki1OKK0v19tmY2deE2usISW8QaoPXxNHnAVdIeppQQ61yP7BfbQeHimFmrxF20d8CbiDsKUBIyg/E5pmxwB8L5hsBXAfcJ6lVuet3xfFLLp1zrkRe43TOuRL5rTMyIuluYO2C4lPN7NEs4nH1R9IZQP+C4hFmNjCLeFz9811155wrke+qO+dciTxxOudciTxxZkjSuZIsMUyTdKekdVJc515xXT3j657x9V4lLOMgSUfVY0ytYwy1LlPSWpKGSfpI0teSPpZ0r6RtE9MMiadpOZcaPziUvXnAbvF5L+B8YLSkjczsywZY/3RgK749F7QYBxGu/R6SRkDVkdQe+B8h3tOBaYRLFPchxP9UQ8XinCfO7C02s//F5/+T9BHhxO09gBGFE0tqZWYL6mvlZraQkJAq3YGE6/E3MbOZifIb83gZqatsvqteeV6Kjz1hWTdmf5d0lqQphM4ekNRE0mmS3pO0UNI7ko5MLkjBuZJmSpov6SbCJX3JaardVZf0q9il2teSPlHoem3V2P3ZAcB2iSaGcxPz9Ytdon0taYakiyU1L1j2ATHeBZKeAjYo4n1pB3wDzCkcUV3HFpJ2jt2vfSnpGUkbFYw/SdILkubF7bs/cYll1TRj4nYPiJ/DAoVu+7oXTNcybufH8bN4TdIeRWyTa6Q8cVaenvFxRqLsF8B2hE54q/rY/BdwJuGuinsCdwM3FCTAPxCuUR9MqLEtIHQ/VitJZxK6KBtL6FDit4QmhdaEpoQnCZcFbhWH6+N8BwF3Ebp124dweeIA4MLEsjcD7gBeA/Yn9AQ1vK6YgJeBFsAwSX1Ue5+TawKXAAOBnwOdgOEFNdMewJWE7t1+RejC7VlJqxYsayvg98CfgGMJXbzdUzDNSOAo4AJgb0KnG/dJ2rSI7XKNUdb92q3MA6EPx08JTSbNgPUJSelzoGucZjKhXa9lYr51CddPH1mwvJuAF+LzpoR2wKsLphkFGNAzvu4ZX+8VX7cjdFl2WS1xjyT2Q5koE/Ahoeu6ZPkxhIS9Wnw9HBhPot9IwnX1BhxVx/t1Wdxui+/RncBOBdMMIfRgtF6ibN84zwY1LLcp0IrQUccRifIxwCJgrUTZ1nFZu8XXO8bX2xUs8yliH6E+5G/wGmf2ViN8ORcROqrtBRxsZtMT04y20PFElR0JCeRuSc2qBmA0sKmkpsAahE4v7i1Y3111xLMVIYncWOJ2rE+o6Q0viOkJQvdrveN0WwD3WcwuRcYEgJn9Ka7nFEJS2w14TLFn/ITJZvZu4vX4+NijqkDSlpJGSZpNSLRfEWrU6xcs62Uz+zARw7PAzLgdADsR9g6ereaz6FvMdrnGxw8OZW8e4ctnhC/gtIKkAqG/zaSOhFrSPKrXFegSn88sGFf4utBq8XF6rVN9V9XN4B6qYfwa8bFLGTEtY2bvEbrMu1ThFhaPETp1vjbxvs0tmO2b+NgSQNKacb5xhB7ip8VpHuS7fWxWF9tMwnsMYbu7EH74Ci2ppszlgCfO7C02s7rOOyxMpHMItaStCTXPQjP59rPtVDCu8HWh2fGxK6EZoVhVB20GENo/C02KjzPKiKlaZvappBuBf8ZlFP7A1GQ3YBWgn8VTvmItsbqOgKuLrRPf/rDMAaYSmgPcSsITZ+P0BKHGuaqZjapuAkkfE5JUP5bvLXz/Opb9HKFN8khCx8DV+Ybv1swmEhJITzO7rpblvwDsI+n0RA2xrpiQtLqF24cUWo/Qk3pNte/qtCL84CxOlB1E9d+HzSStaWYfxTi2JiTOcXH8aOAk4AszK+VcWNeIeeJshMxsoqRrgNsVOiV+kZDINiLcYOyXZrYkjrtU0qeEc0MPAH5Yx7LnSjofGCjpe4Rd7xaEI/fnmdlUwsny/STtC0whNC9Mk3QS4ah3W+BhQoLtRaiNHWhmXwEXAc8T2kL/Q2j7PLaIzT5S0qGEA2CvEW7GtiPhTIOrC9qA61L1w3NjjGEjwo/E3GqmnUnoQPhcwnt8EaHds+rHaBTwKDBK0kWEDojbEnpob2lmp5cQl2sssj46tTIPxKPqdUwzGbi0mnIRenR/i1DjmkU4feiIgmnOj+PmE3qO/wW1HFVPzPtrwkGVhYSa63CWv7Pm3YTdVAPOTcy3OyFJf0k48v0qoSf6Zolp+hPuzPk18AywOXUcVSfcsOzfMab5hCT3EuFUqeSyhwAvFsz7nW0EjgDeJ9Su/wf8uPC9JhyAGkm4DcdHcdqHgTUKlt+CcOrVe4QfixmEWv6eWf+P+ZDO4N3KOVcDSWMIP2wHZh2Lqyx+OpJzzpXIE6dzzpXId9Wdc65EXuN0zrkSeeJ0zrkSeeJ0zrkSeeJ0zrkSeeJ0zrkSeeJ0zrkS/T8oRdHzH8v6cAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#labels and Predicitions\n",
    "truelabels = []\n",
    "predictions = []\n",
    "model.eval()\n",
    "print(\"Getting predictions from test set...\")\n",
    "for data, target in test_loader:\n",
    "    for label in target.data.numpy():\n",
    "        truelabels.append(label)\n",
    "    for prediction in model(data).data.numpy().argmax(1):\n",
    "        predictions.append(prediction) \n",
    "        \n",
    "#Confusion Matrix Graph \n",
    "cm = confusion_matrix(truelabels, predictions)\n",
    "tick_marks = np.arange(len(classes))\n",
    "df_cm = pd.DataFrame(cm, index = classes, columns = classes)\n",
    "plt.figure(figsize = (5,5))\n",
    "sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues, fmt='g')\n",
    "plt.xlabel(\"Predicted Shape\", fontsize = 15)\n",
    "plt.ylabel(\"True Shape\", fontsize = 15)\n",
    "plt.title(\"Confusion Matrix Of Siamese Netural Network\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96359b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
